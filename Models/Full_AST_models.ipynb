{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13012d5-4639-487a-8848-e77db49128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clang.cindex\n",
    "import tempfile\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d6d2111-1c7d-4dfd-8dbc-de8f0e538ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  \n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('Data_Ast.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd74c86e-9dcc-496d-9c46-0251aa198f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "      <th>AST_predicted_values_Random Forest</th>\n",
       "      <th>AST_predicted_values_XGBoost</th>\n",
       "      <th>AST_predicted_values_CatBoost</th>\n",
       "      <th>AST_predicted_values_SVR</th>\n",
       "      <th>AST_predicted_values_KNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.780476</td>\n",
       "      <td>9.158783</td>\n",
       "      <td>8.083245</td>\n",
       "      <td>5.911610</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>5.739167</td>\n",
       "      <td>6.029593</td>\n",
       "      <td>5.678359</td>\n",
       "      <td>6.269147</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>5.064786</td>\n",
       "      <td>5.561528</td>\n",
       "      <td>4.966199</td>\n",
       "      <td>6.273282</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.174000</td>\n",
       "      <td>6.985331</td>\n",
       "      <td>6.514410</td>\n",
       "      <td>6.105001</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.045714</td>\n",
       "      <td>6.096070</td>\n",
       "      <td>5.765330</td>\n",
       "      <td>6.329648</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  \\\n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "\n",
       "   AST_predicted_values_Random Forest  AST_predicted_values_XGBoost  \\\n",
       "0                            7.780476                      9.158783   \n",
       "1                            5.739167                      6.029593   \n",
       "2                            5.064786                      5.561528   \n",
       "3                            7.174000                      6.985331   \n",
       "4                            6.045714                      6.096070   \n",
       "\n",
       "   AST_predicted_values_CatBoost  AST_predicted_values_SVR  \\\n",
       "0                       8.083245                  5.911610   \n",
       "1                       5.678359                  6.269147   \n",
       "2                       4.966199                  6.273282   \n",
       "3                       6.514410                  6.105001   \n",
       "4                       5.765330                  6.329648   \n",
       "\n",
       "   AST_predicted_values_KNN  \n",
       "0                       7.2  \n",
       "1                       5.6  \n",
       "2                       7.2  \n",
       "3                       7.2  \n",
       "4                       5.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "217b24bd-5f34-4040-bd7a-a82c81e62656",
   "metadata": {},
   "outputs": [],
   "source": [
    "Indexes =  [526, 529, 530, 532, 533, 534, 693, 694, 845, 848, 853, 858, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874]\n",
    "data = data.drop(index=Indexes).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acaa98cb-5f30-4bf1-80a9-ff592c3e250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast = torch.load('batch_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a65814c-3e52-4b53-aad1-4bcd64359c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 512, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cbf4ff-87e5-4675-9ec9-b891825efa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined tensor saved as 'combined_batches.pt' with shape: torch.Size([973, 5, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "file_names = [f'batch_{i}.pt' for i in range(10)]  # batch_0.pt, batch_1.pt, ..., batch_8.pt\n",
    "\n",
    "# Initialize an empty list to store the loaded tensors\n",
    "all_tensors = []\n",
    "\n",
    "# Load and append each tensor to the list\n",
    "for file_name in file_names:\n",
    "    tensor = torch.load(file_name)  # Load the tensor from the file\n",
    "    all_tensors.append(tensor)  # Append it to the list\n",
    "\n",
    "# Concatenate all tensors along the batch dimension (assuming they're of the same shape)\n",
    "combined_tensor = torch.cat(all_tensors, dim=0)  # dim=0 means concatenating along the batch dimension\n",
    "\n",
    "# Save the combined tensor to a new .pt file\n",
    "torch.save(combined_tensor, 'combined_batches.pt')\n",
    "\n",
    "print(f\"Combined tensor saved as 'combined_batches.pt' with shape: {combined_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903f44d0-e95d-4310-bc08-c95708e5a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS tokens shape: torch.Size([973, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract the CLS token (first token from the sequence, i.e., index 0 along dimension 2)\n",
    "cls_tokens = combined_tensor[:, :, 0, :]  # Shape: [100, 5, 768]\n",
    "\n",
    "print(f\"CLS tokens shape: {cls_tokens.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeab529-2ce1-45ad-af8a-974b43f16436",
   "metadata": {},
   "source": [
    "## BiLSTM + Neural Nets linear regression layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e11436df-f971-453b-8320-4f993e686a4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded. Shape: torch.Size([973, 768])\n",
      "Fold 1\n",
      "Epoch 1/100, Loss: 3.2354\n",
      "Epoch 2/100, Loss: 1.3965\n",
      "Epoch 3/100, Loss: 1.3665\n",
      "Epoch 4/100, Loss: 1.2825\n",
      "Epoch 5/100, Loss: 1.1965\n",
      "Epoch 6/100, Loss: 1.2481\n",
      "Epoch 7/100, Loss: 1.1626\n",
      "Epoch 8/100, Loss: 1.1549\n",
      "Epoch 9/100, Loss: 1.1380\n",
      "Epoch 10/100, Loss: 1.0893\n",
      "Epoch 11/100, Loss: 1.0724\n",
      "Epoch 12/100, Loss: 1.0293\n",
      "Epoch 13/100, Loss: 0.9813\n",
      "Epoch 14/100, Loss: 1.0171\n",
      "Epoch 15/100, Loss: 0.9670\n",
      "Epoch 16/100, Loss: 0.9318\n",
      "Epoch 17/100, Loss: 0.9158\n",
      "Epoch 18/100, Loss: 0.9983\n",
      "Epoch 19/100, Loss: 0.9989\n",
      "Epoch 20/100, Loss: 0.9211\n",
      "Epoch 21/100, Loss: 0.8846\n",
      "Epoch 22/100, Loss: 0.8875\n",
      "Epoch 23/100, Loss: 0.8855\n",
      "Epoch 24/100, Loss: 0.8379\n",
      "Epoch 25/100, Loss: 0.8323\n",
      "Epoch 26/100, Loss: 0.8296\n",
      "Epoch 27/100, Loss: 0.8055\n",
      "Epoch 28/100, Loss: 0.7992\n",
      "Epoch 29/100, Loss: 0.8672\n",
      "Epoch 30/100, Loss: 0.8434\n",
      "Epoch 31/100, Loss: 0.8018\n",
      "Epoch 32/100, Loss: 0.8040\n",
      "Epoch 33/100, Loss: 0.7676\n",
      "Epoch 34/100, Loss: 0.7329\n",
      "Epoch 35/100, Loss: 0.7682\n",
      "Epoch 36/100, Loss: 0.7528\n",
      "Epoch 37/100, Loss: 0.7252\n",
      "Epoch 38/100, Loss: 0.7437\n",
      "Epoch 39/100, Loss: 0.7164\n",
      "Epoch 40/100, Loss: 0.7442\n",
      "Epoch 41/100, Loss: 0.7222\n",
      "Epoch 42/100, Loss: 0.7062\n",
      "Epoch 43/100, Loss: 0.6867\n",
      "Epoch 44/100, Loss: 0.7219\n",
      "Epoch 45/100, Loss: 0.6910\n",
      "Epoch 46/100, Loss: 0.6765\n",
      "Epoch 47/100, Loss: 0.6945\n",
      "Epoch 48/100, Loss: 0.7047\n",
      "Epoch 49/100, Loss: 0.7216\n",
      "Epoch 50/100, Loss: 0.6667\n",
      "Epoch 51/100, Loss: 0.6577\n",
      "Epoch 52/100, Loss: 0.6554\n",
      "Epoch 53/100, Loss: 0.6461\n",
      "Epoch 54/100, Loss: 0.6895\n",
      "Epoch 55/100, Loss: 0.6872\n",
      "Epoch 56/100, Loss: 0.7225\n",
      "Epoch 57/100, Loss: 0.6772\n",
      "Epoch 58/100, Loss: 0.6565\n",
      "Epoch 59/100, Loss: 0.6737\n",
      "Epoch 60/100, Loss: 0.6507\n",
      "Epoch 61/100, Loss: 0.7348\n",
      "Epoch 62/100, Loss: 0.6778\n",
      "Epoch 63/100, Loss: 0.6514\n",
      "Epoch 64/100, Loss: 0.6452\n",
      "Epoch 65/100, Loss: 0.6398\n",
      "Epoch 66/100, Loss: 0.6647\n",
      "Epoch 67/100, Loss: 0.6516\n",
      "Epoch 68/100, Loss: 0.6696\n",
      "Epoch 69/100, Loss: 0.6485\n",
      "Epoch 70/100, Loss: 0.6613\n",
      "Epoch 71/100, Loss: 0.6382\n",
      "Epoch 72/100, Loss: 0.6288\n",
      "Epoch 73/100, Loss: 0.6419\n",
      "Epoch 74/100, Loss: 0.6342\n",
      "Epoch 75/100, Loss: 0.6403\n",
      "Epoch 76/100, Loss: 0.6289\n",
      "Epoch 77/100, Loss: 0.6289\n",
      "Epoch 78/100, Loss: 0.6423\n",
      "Epoch 79/100, Loss: 0.6292\n",
      "Epoch 80/100, Loss: 0.6523\n",
      "Epoch 81/100, Loss: 0.6612\n",
      "Epoch 82/100, Loss: 0.6079\n",
      "Epoch 83/100, Loss: 0.6386\n",
      "Epoch 84/100, Loss: 0.6767\n",
      "Epoch 85/100, Loss: 0.6247\n",
      "Epoch 86/100, Loss: 0.6138\n",
      "Epoch 87/100, Loss: 0.6329\n",
      "Epoch 88/100, Loss: 0.6151\n",
      "Epoch 89/100, Loss: 0.6213\n",
      "Epoch 90/100, Loss: 0.6572\n",
      "Epoch 91/100, Loss: 0.6267\n",
      "Epoch 92/100, Loss: 0.6167\n",
      "Epoch 93/100, Loss: 0.6112\n",
      "Epoch 94/100, Loss: 0.6248\n",
      "Epoch 95/100, Loss: 0.6435\n",
      "Epoch 96/100, Loss: 0.5995\n",
      "Epoch 97/100, Loss: 0.6025\n",
      "Epoch 98/100, Loss: 0.6086\n",
      "Epoch 99/100, Loss: 0.6225\n",
      "Epoch 100/100, Loss: 0.6485\n",
      "Fold 1 R²: 0.2184, RMSE: 2.0865, MAPE: 127429758681088.0000\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.9405\n",
      "Epoch 2/100, Loss: 1.4375\n",
      "Epoch 3/100, Loss: 1.2799\n",
      "Epoch 4/100, Loss: 1.2531\n",
      "Epoch 5/100, Loss: 1.1997\n",
      "Epoch 6/100, Loss: 1.2087\n",
      "Epoch 7/100, Loss: 1.1147\n",
      "Epoch 8/100, Loss: 1.1536\n",
      "Epoch 9/100, Loss: 1.0298\n",
      "Epoch 10/100, Loss: 0.9896\n",
      "Epoch 11/100, Loss: 1.0303\n",
      "Epoch 12/100, Loss: 1.0007\n",
      "Epoch 13/100, Loss: 0.9424\n",
      "Epoch 14/100, Loss: 0.9175\n",
      "Epoch 15/100, Loss: 0.9309\n",
      "Epoch 16/100, Loss: 0.8980\n",
      "Epoch 17/100, Loss: 0.8775\n",
      "Epoch 18/100, Loss: 0.8497\n",
      "Epoch 19/100, Loss: 0.8582\n",
      "Epoch 20/100, Loss: 0.8548\n",
      "Epoch 21/100, Loss: 0.8725\n",
      "Epoch 22/100, Loss: 0.8353\n",
      "Epoch 23/100, Loss: 0.8215\n",
      "Epoch 24/100, Loss: 0.8016\n",
      "Epoch 25/100, Loss: 0.8167\n",
      "Epoch 26/100, Loss: 0.8983\n",
      "Epoch 27/100, Loss: 0.8267\n",
      "Epoch 28/100, Loss: 0.7602\n",
      "Epoch 29/100, Loss: 0.7338\n",
      "Epoch 30/100, Loss: 0.7362\n",
      "Epoch 31/100, Loss: 0.7491\n",
      "Epoch 32/100, Loss: 0.7996\n",
      "Epoch 33/100, Loss: 0.7576\n",
      "Epoch 34/100, Loss: 0.7038\n",
      "Epoch 35/100, Loss: 0.7041\n",
      "Epoch 36/100, Loss: 0.6961\n",
      "Epoch 37/100, Loss: 0.6843\n",
      "Epoch 38/100, Loss: 0.6776\n",
      "Epoch 39/100, Loss: 0.6767\n",
      "Epoch 40/100, Loss: 0.7039\n",
      "Epoch 41/100, Loss: 0.7267\n",
      "Epoch 42/100, Loss: 0.6742\n",
      "Epoch 43/100, Loss: 0.7122\n",
      "Epoch 44/100, Loss: 0.6550\n",
      "Epoch 45/100, Loss: 0.6848\n",
      "Epoch 46/100, Loss: 0.6693\n",
      "Epoch 47/100, Loss: 0.6491\n",
      "Epoch 48/100, Loss: 0.7164\n",
      "Epoch 49/100, Loss: 0.7117\n",
      "Epoch 50/100, Loss: 0.6673\n",
      "Epoch 51/100, Loss: 0.6173\n",
      "Epoch 52/100, Loss: 0.6610\n",
      "Epoch 53/100, Loss: 0.6359\n",
      "Epoch 54/100, Loss: 0.6177\n",
      "Epoch 55/100, Loss: 0.6423\n",
      "Epoch 56/100, Loss: 0.6244\n",
      "Epoch 57/100, Loss: 0.6353\n",
      "Epoch 58/100, Loss: 0.6280\n",
      "Epoch 59/100, Loss: 0.6638\n",
      "Epoch 60/100, Loss: 0.6225\n",
      "Epoch 61/100, Loss: 0.6323\n",
      "Epoch 62/100, Loss: 0.6807\n",
      "Epoch 63/100, Loss: 0.6895\n",
      "Epoch 64/100, Loss: 0.6268\n",
      "Epoch 65/100, Loss: 0.6596\n",
      "Epoch 66/100, Loss: 0.6777\n",
      "Epoch 67/100, Loss: 0.6345\n",
      "Epoch 68/100, Loss: 0.6567\n",
      "Epoch 69/100, Loss: 0.6452\n",
      "Epoch 70/100, Loss: 0.6148\n",
      "Epoch 71/100, Loss: 0.6170\n",
      "Epoch 72/100, Loss: 0.6104\n",
      "Epoch 73/100, Loss: 0.6270\n",
      "Epoch 74/100, Loss: 0.6089\n",
      "Epoch 75/100, Loss: 0.6255\n",
      "Epoch 76/100, Loss: 0.5943\n",
      "Epoch 77/100, Loss: 0.5941\n",
      "Epoch 78/100, Loss: 0.6152\n",
      "Epoch 79/100, Loss: 0.6000\n",
      "Epoch 80/100, Loss: 0.5985\n",
      "Epoch 81/100, Loss: 0.5875\n",
      "Epoch 82/100, Loss: 0.5803\n",
      "Epoch 83/100, Loss: 0.5962\n",
      "Epoch 84/100, Loss: 0.6203\n",
      "Epoch 85/100, Loss: 0.6150\n",
      "Epoch 86/100, Loss: 0.5931\n",
      "Epoch 87/100, Loss: 0.5929\n",
      "Epoch 88/100, Loss: 0.6187\n",
      "Epoch 89/100, Loss: 0.6096\n",
      "Epoch 90/100, Loss: 0.6135\n",
      "Epoch 91/100, Loss: 0.6176\n",
      "Epoch 92/100, Loss: 0.5780\n",
      "Epoch 93/100, Loss: 0.5980\n",
      "Epoch 94/100, Loss: 0.5918\n",
      "Epoch 95/100, Loss: 0.5631\n",
      "Epoch 96/100, Loss: 0.6013\n",
      "Epoch 97/100, Loss: 0.6034\n",
      "Epoch 98/100, Loss: 0.6105\n",
      "Epoch 99/100, Loss: 0.5956\n",
      "Epoch 100/100, Loss: 0.6117\n",
      "Fold 2 R²: 0.0050, RMSE: 2.3751, MAPE: 0.4540\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.8067\n",
      "Epoch 2/100, Loss: 1.3950\n",
      "Epoch 3/100, Loss: 1.3437\n",
      "Epoch 4/100, Loss: 1.2987\n",
      "Epoch 5/100, Loss: 1.2787\n",
      "Epoch 6/100, Loss: 1.2167\n",
      "Epoch 7/100, Loss: 1.1649\n",
      "Epoch 8/100, Loss: 1.1814\n",
      "Epoch 9/100, Loss: 1.0857\n",
      "Epoch 10/100, Loss: 1.0255\n",
      "Epoch 11/100, Loss: 1.1119\n",
      "Epoch 12/100, Loss: 0.9973\n",
      "Epoch 13/100, Loss: 0.9960\n",
      "Epoch 14/100, Loss: 0.9884\n",
      "Epoch 15/100, Loss: 0.9961\n",
      "Epoch 16/100, Loss: 0.9720\n",
      "Epoch 17/100, Loss: 0.9089\n",
      "Epoch 18/100, Loss: 0.9363\n",
      "Epoch 19/100, Loss: 0.9535\n",
      "Epoch 20/100, Loss: 0.8994\n",
      "Epoch 21/100, Loss: 0.8976\n",
      "Epoch 22/100, Loss: 0.8588\n",
      "Epoch 23/100, Loss: 0.8524\n",
      "Epoch 24/100, Loss: 0.9029\n",
      "Epoch 25/100, Loss: 0.8425\n",
      "Epoch 26/100, Loss: 0.8489\n",
      "Epoch 27/100, Loss: 0.8565\n",
      "Epoch 28/100, Loss: 0.8397\n",
      "Epoch 29/100, Loss: 0.8273\n",
      "Epoch 30/100, Loss: 0.8105\n",
      "Epoch 31/100, Loss: 0.8218\n",
      "Epoch 32/100, Loss: 0.8550\n",
      "Epoch 33/100, Loss: 0.8002\n",
      "Epoch 34/100, Loss: 0.7552\n",
      "Epoch 35/100, Loss: 0.7393\n",
      "Epoch 36/100, Loss: 0.7447\n",
      "Epoch 37/100, Loss: 0.7512\n",
      "Epoch 38/100, Loss: 0.7544\n",
      "Epoch 39/100, Loss: 0.7569\n",
      "Epoch 40/100, Loss: 0.7505\n",
      "Epoch 41/100, Loss: 0.8026\n",
      "Epoch 42/100, Loss: 0.7640\n",
      "Epoch 43/100, Loss: 0.7047\n",
      "Epoch 44/100, Loss: 0.7454\n",
      "Epoch 45/100, Loss: 0.6995\n",
      "Epoch 46/100, Loss: 0.6981\n",
      "Epoch 47/100, Loss: 0.7014\n",
      "Epoch 48/100, Loss: 0.6961\n",
      "Epoch 49/100, Loss: 0.6822\n",
      "Epoch 50/100, Loss: 0.6727\n",
      "Epoch 51/100, Loss: 0.6872\n",
      "Epoch 52/100, Loss: 0.7196\n",
      "Epoch 53/100, Loss: 0.7048\n",
      "Epoch 54/100, Loss: 0.7332\n",
      "Epoch 55/100, Loss: 0.6675\n",
      "Epoch 56/100, Loss: 0.6731\n",
      "Epoch 57/100, Loss: 0.6460\n",
      "Epoch 58/100, Loss: 0.6635\n",
      "Epoch 59/100, Loss: 0.6824\n",
      "Epoch 60/100, Loss: 0.6746\n",
      "Epoch 61/100, Loss: 0.6521\n",
      "Epoch 62/100, Loss: 0.6427\n",
      "Epoch 63/100, Loss: 0.7010\n",
      "Epoch 64/100, Loss: 0.6653\n",
      "Epoch 65/100, Loss: 0.6675\n",
      "Epoch 66/100, Loss: 0.6460\n",
      "Epoch 67/100, Loss: 0.6495\n",
      "Epoch 68/100, Loss: 0.6544\n",
      "Epoch 69/100, Loss: 0.6780\n",
      "Epoch 70/100, Loss: 0.6474\n",
      "Epoch 71/100, Loss: 0.6508\n",
      "Epoch 72/100, Loss: 0.6297\n",
      "Epoch 73/100, Loss: 0.6601\n",
      "Epoch 74/100, Loss: 0.6695\n",
      "Epoch 75/100, Loss: 0.6511\n",
      "Epoch 76/100, Loss: 0.6508\n",
      "Epoch 77/100, Loss: 0.6585\n",
      "Epoch 78/100, Loss: 0.6308\n",
      "Epoch 79/100, Loss: 0.6343\n",
      "Epoch 80/100, Loss: 0.6745\n",
      "Epoch 81/100, Loss: 0.6456\n",
      "Epoch 82/100, Loss: 0.6630\n",
      "Epoch 83/100, Loss: 0.6469\n",
      "Epoch 84/100, Loss: 0.6326\n",
      "Epoch 85/100, Loss: 0.6532\n",
      "Epoch 86/100, Loss: 0.6383\n",
      "Epoch 87/100, Loss: 0.6218\n",
      "Epoch 88/100, Loss: 0.6260\n",
      "Epoch 89/100, Loss: 0.6536\n",
      "Epoch 90/100, Loss: 0.6387\n",
      "Epoch 91/100, Loss: 0.6046\n",
      "Epoch 92/100, Loss: 0.6114\n",
      "Epoch 93/100, Loss: 0.6141\n",
      "Epoch 94/100, Loss: 0.6129\n",
      "Epoch 95/100, Loss: 0.6208\n",
      "Epoch 96/100, Loss: 0.6439\n",
      "Epoch 97/100, Loss: 0.6843\n",
      "Epoch 98/100, Loss: 0.6272\n",
      "Epoch 99/100, Loss: 0.6273\n",
      "Epoch 100/100, Loss: 0.6024\n",
      "Fold 3 R²: 0.1957, RMSE: 2.1399, MAPE: 0.4133\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.2641\n",
      "Epoch 2/100, Loss: 1.4353\n",
      "Epoch 3/100, Loss: 1.3565\n",
      "Epoch 4/100, Loss: 1.2571\n",
      "Epoch 5/100, Loss: 1.2320\n",
      "Epoch 6/100, Loss: 1.1816\n",
      "Epoch 7/100, Loss: 1.1748\n",
      "Epoch 8/100, Loss: 1.0734\n",
      "Epoch 9/100, Loss: 1.0552\n",
      "Epoch 10/100, Loss: 1.0324\n",
      "Epoch 11/100, Loss: 1.0625\n",
      "Epoch 12/100, Loss: 0.9832\n",
      "Epoch 13/100, Loss: 0.9660\n",
      "Epoch 14/100, Loss: 0.9994\n",
      "Epoch 15/100, Loss: 0.9537\n",
      "Epoch 16/100, Loss: 0.9113\n",
      "Epoch 17/100, Loss: 0.8916\n",
      "Epoch 18/100, Loss: 0.8780\n",
      "Epoch 19/100, Loss: 0.8761\n",
      "Epoch 20/100, Loss: 0.8375\n",
      "Epoch 21/100, Loss: 0.8274\n",
      "Epoch 22/100, Loss: 0.8551\n",
      "Epoch 23/100, Loss: 0.8257\n",
      "Epoch 24/100, Loss: 0.7971\n",
      "Epoch 25/100, Loss: 0.8248\n",
      "Epoch 26/100, Loss: 0.8339\n",
      "Epoch 27/100, Loss: 0.7849\n",
      "Epoch 28/100, Loss: 0.7844\n",
      "Epoch 29/100, Loss: 0.7679\n",
      "Epoch 30/100, Loss: 0.7979\n",
      "Epoch 31/100, Loss: 0.7587\n",
      "Epoch 32/100, Loss: 0.7495\n",
      "Epoch 33/100, Loss: 0.7237\n",
      "Epoch 34/100, Loss: 0.7124\n",
      "Epoch 35/100, Loss: 0.7137\n",
      "Epoch 36/100, Loss: 0.7361\n",
      "Epoch 37/100, Loss: 0.7434\n",
      "Epoch 38/100, Loss: 0.7307\n",
      "Epoch 39/100, Loss: 0.7656\n",
      "Epoch 40/100, Loss: 0.7855\n",
      "Epoch 41/100, Loss: 0.7034\n",
      "Epoch 42/100, Loss: 0.7110\n",
      "Epoch 43/100, Loss: 0.7110\n",
      "Epoch 44/100, Loss: 0.6715\n",
      "Epoch 45/100, Loss: 0.6713\n",
      "Epoch 46/100, Loss: 0.6516\n",
      "Epoch 47/100, Loss: 0.6638\n",
      "Epoch 48/100, Loss: 0.6672\n",
      "Epoch 49/100, Loss: 0.6762\n",
      "Epoch 50/100, Loss: 0.6392\n",
      "Epoch 51/100, Loss: 0.6860\n",
      "Epoch 52/100, Loss: 0.6679\n",
      "Epoch 53/100, Loss: 0.6737\n",
      "Epoch 54/100, Loss: 0.6419\n",
      "Epoch 55/100, Loss: 0.6671\n",
      "Epoch 56/100, Loss: 0.6230\n",
      "Epoch 57/100, Loss: 0.6543\n",
      "Epoch 58/100, Loss: 0.6393\n",
      "Epoch 59/100, Loss: 0.6333\n",
      "Epoch 60/100, Loss: 0.6523\n",
      "Epoch 61/100, Loss: 0.6477\n",
      "Epoch 62/100, Loss: 0.6760\n",
      "Epoch 63/100, Loss: 0.6173\n",
      "Epoch 64/100, Loss: 0.6568\n",
      "Epoch 65/100, Loss: 0.6464\n",
      "Epoch 66/100, Loss: 0.6284\n",
      "Epoch 67/100, Loss: 0.6129\n",
      "Epoch 68/100, Loss: 0.6230\n",
      "Epoch 69/100, Loss: 0.6377\n",
      "Epoch 70/100, Loss: 0.6475\n",
      "Epoch 71/100, Loss: 0.6106\n",
      "Epoch 72/100, Loss: 0.6174\n",
      "Epoch 73/100, Loss: 0.6336\n",
      "Epoch 74/100, Loss: 0.6249\n",
      "Epoch 75/100, Loss: 0.6784\n",
      "Epoch 76/100, Loss: 0.6292\n",
      "Epoch 77/100, Loss: 0.6242\n",
      "Epoch 78/100, Loss: 0.5942\n",
      "Epoch 79/100, Loss: 0.5997\n",
      "Epoch 80/100, Loss: 0.6245\n",
      "Epoch 81/100, Loss: 0.6237\n",
      "Epoch 82/100, Loss: 0.6200\n",
      "Epoch 83/100, Loss: 0.5978\n",
      "Epoch 84/100, Loss: 0.6339\n",
      "Epoch 85/100, Loss: 0.6074\n",
      "Epoch 86/100, Loss: 0.6169\n",
      "Epoch 87/100, Loss: 0.6219\n",
      "Epoch 88/100, Loss: 0.6171\n",
      "Epoch 89/100, Loss: 0.5854\n",
      "Epoch 90/100, Loss: 0.5942\n",
      "Epoch 91/100, Loss: 0.5765\n",
      "Epoch 92/100, Loss: 0.5840\n",
      "Epoch 93/100, Loss: 0.6090\n",
      "Epoch 94/100, Loss: 0.5887\n",
      "Epoch 95/100, Loss: 0.5802\n",
      "Epoch 96/100, Loss: 0.5844\n",
      "Epoch 97/100, Loss: 0.5898\n",
      "Epoch 98/100, Loss: 0.5880\n",
      "Epoch 99/100, Loss: 0.6006\n",
      "Epoch 100/100, Loss: 0.6056\n",
      "Fold 4 R²: -0.2379, RMSE: 2.3807, MAPE: 0.4034\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.2842\n",
      "Epoch 2/100, Loss: 1.5032\n",
      "Epoch 3/100, Loss: 1.3465\n",
      "Epoch 4/100, Loss: 1.2664\n",
      "Epoch 5/100, Loss: 1.2236\n",
      "Epoch 6/100, Loss: 1.2080\n",
      "Epoch 7/100, Loss: 1.1910\n",
      "Epoch 8/100, Loss: 1.1361\n",
      "Epoch 9/100, Loss: 1.1058\n",
      "Epoch 10/100, Loss: 1.0420\n",
      "Epoch 11/100, Loss: 1.1302\n",
      "Epoch 12/100, Loss: 1.0957\n",
      "Epoch 13/100, Loss: 1.0201\n",
      "Epoch 14/100, Loss: 1.0754\n",
      "Epoch 15/100, Loss: 0.9573\n",
      "Epoch 16/100, Loss: 0.9644\n",
      "Epoch 17/100, Loss: 0.9527\n",
      "Epoch 18/100, Loss: 0.9232\n",
      "Epoch 19/100, Loss: 0.9918\n",
      "Epoch 20/100, Loss: 0.9240\n",
      "Epoch 21/100, Loss: 0.9128\n",
      "Epoch 22/100, Loss: 0.8794\n",
      "Epoch 23/100, Loss: 0.8540\n",
      "Epoch 24/100, Loss: 0.8687\n",
      "Epoch 25/100, Loss: 0.8408\n",
      "Epoch 26/100, Loss: 0.8199\n",
      "Epoch 27/100, Loss: 0.8516\n",
      "Epoch 28/100, Loss: 0.8424\n",
      "Epoch 29/100, Loss: 0.8288\n",
      "Epoch 30/100, Loss: 0.8578\n",
      "Epoch 31/100, Loss: 0.8280\n",
      "Epoch 32/100, Loss: 0.7945\n",
      "Epoch 33/100, Loss: 0.7939\n",
      "Epoch 34/100, Loss: 0.7846\n",
      "Epoch 35/100, Loss: 0.7751\n",
      "Epoch 36/100, Loss: 0.7378\n",
      "Epoch 37/100, Loss: 0.7620\n",
      "Epoch 38/100, Loss: 0.7482\n",
      "Epoch 39/100, Loss: 0.7555\n",
      "Epoch 40/100, Loss: 0.7700\n",
      "Epoch 41/100, Loss: 0.7628\n",
      "Epoch 42/100, Loss: 0.7180\n",
      "Epoch 43/100, Loss: 0.7604\n",
      "Epoch 44/100, Loss: 0.7729\n",
      "Epoch 45/100, Loss: 0.7638\n",
      "Epoch 46/100, Loss: 0.7405\n",
      "Epoch 47/100, Loss: 0.6989\n",
      "Epoch 48/100, Loss: 0.6906\n",
      "Epoch 49/100, Loss: 0.6691\n",
      "Epoch 50/100, Loss: 0.6816\n",
      "Epoch 51/100, Loss: 0.7305\n",
      "Epoch 52/100, Loss: 0.6994\n",
      "Epoch 53/100, Loss: 0.7169\n",
      "Epoch 54/100, Loss: 0.7270\n",
      "Epoch 55/100, Loss: 0.7049\n",
      "Epoch 56/100, Loss: 0.6764\n",
      "Epoch 57/100, Loss: 0.6947\n",
      "Epoch 58/100, Loss: 0.6605\n",
      "Epoch 59/100, Loss: 0.6634\n",
      "Epoch 60/100, Loss: 0.6668\n",
      "Epoch 61/100, Loss: 0.6809\n",
      "Epoch 62/100, Loss: 0.6620\n",
      "Epoch 63/100, Loss: 0.6808\n",
      "Epoch 64/100, Loss: 0.6664\n",
      "Epoch 65/100, Loss: 0.6693\n",
      "Epoch 66/100, Loss: 0.6418\n",
      "Epoch 67/100, Loss: 0.6445\n",
      "Epoch 68/100, Loss: 0.6246\n",
      "Epoch 69/100, Loss: 0.6483\n",
      "Epoch 70/100, Loss: 0.6265\n",
      "Epoch 71/100, Loss: 0.6316\n",
      "Epoch 72/100, Loss: 0.6447\n",
      "Epoch 73/100, Loss: 0.6322\n",
      "Epoch 74/100, Loss: 0.6635\n",
      "Epoch 75/100, Loss: 0.6349\n",
      "Epoch 76/100, Loss: 0.6604\n",
      "Epoch 77/100, Loss: 0.6460\n",
      "Epoch 78/100, Loss: 0.6346\n",
      "Epoch 79/100, Loss: 0.6358\n",
      "Epoch 80/100, Loss: 0.6431\n",
      "Epoch 81/100, Loss: 0.6295\n",
      "Epoch 82/100, Loss: 0.6207\n",
      "Epoch 83/100, Loss: 0.6324\n",
      "Epoch 84/100, Loss: 0.6128\n",
      "Epoch 85/100, Loss: 0.6130\n",
      "Epoch 86/100, Loss: 0.6323\n",
      "Epoch 87/100, Loss: 0.6043\n",
      "Epoch 88/100, Loss: 0.6163\n",
      "Epoch 89/100, Loss: 0.6211\n",
      "Epoch 90/100, Loss: 0.6089\n",
      "Epoch 91/100, Loss: 0.6122\n",
      "Epoch 92/100, Loss: 0.6020\n",
      "Epoch 93/100, Loss: 0.5991\n",
      "Epoch 94/100, Loss: 0.6215\n",
      "Epoch 95/100, Loss: 0.6048\n",
      "Epoch 96/100, Loss: 0.6074\n",
      "Epoch 97/100, Loss: 0.6187\n",
      "Epoch 98/100, Loss: 0.6126\n",
      "Epoch 99/100, Loss: 0.6320\n",
      "Epoch 100/100, Loss: 0.6113\n",
      "Fold 5 R²: 0.0423, RMSE: 2.1311, MAPE: 191208085258240.0000\n",
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.7169\n",
      "Epoch 2/100, Loss: 1.4263\n",
      "Epoch 3/100, Loss: 1.3380\n",
      "Epoch 4/100, Loss: 1.3681\n",
      "Epoch 5/100, Loss: 1.2913\n",
      "Epoch 6/100, Loss: 1.2129\n",
      "Epoch 7/100, Loss: 1.1622\n",
      "Epoch 8/100, Loss: 1.1018\n",
      "Epoch 9/100, Loss: 1.0562\n",
      "Epoch 10/100, Loss: 1.0700\n",
      "Epoch 11/100, Loss: 1.0507\n",
      "Epoch 12/100, Loss: 1.0479\n",
      "Epoch 13/100, Loss: 0.9874\n",
      "Epoch 14/100, Loss: 0.9418\n",
      "Epoch 15/100, Loss: 1.0182\n",
      "Epoch 16/100, Loss: 1.0002\n",
      "Epoch 17/100, Loss: 1.0172\n",
      "Epoch 18/100, Loss: 0.9267\n",
      "Epoch 19/100, Loss: 0.8939\n",
      "Epoch 20/100, Loss: 0.9554\n",
      "Epoch 21/100, Loss: 0.9328\n",
      "Epoch 22/100, Loss: 0.8826\n",
      "Epoch 23/100, Loss: 0.8442\n",
      "Epoch 24/100, Loss: 0.8357\n",
      "Epoch 25/100, Loss: 0.8231\n",
      "Epoch 26/100, Loss: 0.8267\n",
      "Epoch 27/100, Loss: 0.8488\n",
      "Epoch 28/100, Loss: 0.7899\n",
      "Epoch 29/100, Loss: 0.8015\n",
      "Epoch 30/100, Loss: 0.8368\n",
      "Epoch 31/100, Loss: 0.7779\n",
      "Epoch 32/100, Loss: 0.7894\n",
      "Epoch 33/100, Loss: 0.7766\n",
      "Epoch 34/100, Loss: 0.7823\n",
      "Epoch 35/100, Loss: 0.7661\n",
      "Epoch 36/100, Loss: 0.8021\n",
      "Epoch 37/100, Loss: 0.7668\n",
      "Epoch 38/100, Loss: 0.7212\n",
      "Epoch 39/100, Loss: 0.7397\n",
      "Epoch 40/100, Loss: 0.7161\n",
      "Epoch 41/100, Loss: 0.7055\n",
      "Epoch 42/100, Loss: 0.6952\n",
      "Epoch 43/100, Loss: 0.6736\n",
      "Epoch 44/100, Loss: 0.6827\n",
      "Epoch 45/100, Loss: 0.6784\n",
      "Epoch 46/100, Loss: 0.7083\n",
      "Epoch 47/100, Loss: 0.6732\n",
      "Epoch 48/100, Loss: 0.6595\n",
      "Epoch 49/100, Loss: 0.6959\n",
      "Epoch 50/100, Loss: 0.6580\n",
      "Epoch 51/100, Loss: 0.6738\n",
      "Epoch 52/100, Loss: 0.6964\n",
      "Epoch 53/100, Loss: 0.7057\n",
      "Epoch 54/100, Loss: 0.6780\n",
      "Epoch 55/100, Loss: 0.6700\n",
      "Epoch 56/100, Loss: 0.7040\n",
      "Epoch 57/100, Loss: 0.6598\n",
      "Epoch 58/100, Loss: 0.6691\n",
      "Epoch 59/100, Loss: 0.6487\n",
      "Epoch 60/100, Loss: 0.6577\n",
      "Epoch 61/100, Loss: 0.6480\n",
      "Epoch 62/100, Loss: 0.6620\n",
      "Epoch 63/100, Loss: 0.6657\n",
      "Epoch 64/100, Loss: 0.6531\n",
      "Epoch 65/100, Loss: 0.6532\n",
      "Epoch 66/100, Loss: 0.6314\n",
      "Epoch 67/100, Loss: 0.6639\n",
      "Epoch 68/100, Loss: 0.6519\n",
      "Epoch 69/100, Loss: 0.6366\n",
      "Epoch 70/100, Loss: 0.6868\n",
      "Epoch 71/100, Loss: 0.6253\n",
      "Epoch 72/100, Loss: 0.6315\n",
      "Epoch 73/100, Loss: 0.6191\n",
      "Epoch 74/100, Loss: 0.6388\n",
      "Epoch 75/100, Loss: 0.6519\n",
      "Epoch 76/100, Loss: 0.6847\n",
      "Epoch 77/100, Loss: 0.6395\n",
      "Epoch 78/100, Loss: 0.6140\n",
      "Epoch 79/100, Loss: 0.6230\n",
      "Epoch 80/100, Loss: 0.6236\n",
      "Epoch 81/100, Loss: 0.6318\n",
      "Epoch 82/100, Loss: 0.6554\n",
      "Epoch 83/100, Loss: 0.6363\n",
      "Epoch 84/100, Loss: 0.6465\n",
      "Epoch 85/100, Loss: 0.6120\n",
      "Epoch 86/100, Loss: 0.6315\n",
      "Epoch 87/100, Loss: 0.6524\n",
      "Epoch 88/100, Loss: 0.6359\n",
      "Epoch 89/100, Loss: 0.6361\n",
      "Epoch 90/100, Loss: 0.6175\n",
      "Epoch 91/100, Loss: 0.6232\n",
      "Epoch 92/100, Loss: 0.6231\n",
      "Epoch 93/100, Loss: 0.6245\n",
      "Epoch 94/100, Loss: 0.6656\n",
      "Epoch 95/100, Loss: 0.6194\n",
      "Epoch 96/100, Loss: 0.6205\n",
      "Epoch 97/100, Loss: 0.6226\n",
      "Epoch 98/100, Loss: 0.6440\n",
      "Epoch 99/100, Loss: 0.5901\n",
      "Epoch 100/100, Loss: 0.6133\n",
      "Fold 6 R²: 0.1325, RMSE: 2.1427, MAPE: 0.5160\n",
      "Fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.2997\n",
      "Epoch 2/100, Loss: 1.5070\n",
      "Epoch 3/100, Loss: 1.3255\n",
      "Epoch 4/100, Loss: 1.3338\n",
      "Epoch 5/100, Loss: 1.2918\n",
      "Epoch 6/100, Loss: 1.2278\n",
      "Epoch 7/100, Loss: 1.1629\n",
      "Epoch 8/100, Loss: 1.1631\n",
      "Epoch 9/100, Loss: 1.1218\n",
      "Epoch 10/100, Loss: 1.0586\n",
      "Epoch 11/100, Loss: 1.1132\n",
      "Epoch 12/100, Loss: 1.0339\n",
      "Epoch 13/100, Loss: 0.9767\n",
      "Epoch 14/100, Loss: 1.0146\n",
      "Epoch 15/100, Loss: 1.0247\n",
      "Epoch 16/100, Loss: 0.9916\n",
      "Epoch 17/100, Loss: 0.9582\n",
      "Epoch 18/100, Loss: 1.0240\n",
      "Epoch 19/100, Loss: 0.9711\n",
      "Epoch 20/100, Loss: 0.9353\n",
      "Epoch 21/100, Loss: 0.9174\n",
      "Epoch 22/100, Loss: 0.8970\n",
      "Epoch 23/100, Loss: 0.8729\n",
      "Epoch 24/100, Loss: 0.8636\n",
      "Epoch 25/100, Loss: 0.8271\n",
      "Epoch 26/100, Loss: 0.8270\n",
      "Epoch 27/100, Loss: 0.8542\n",
      "Epoch 28/100, Loss: 0.8017\n",
      "Epoch 29/100, Loss: 0.8200\n",
      "Epoch 30/100, Loss: 0.7948\n",
      "Epoch 31/100, Loss: 0.7474\n",
      "Epoch 32/100, Loss: 0.7654\n",
      "Epoch 33/100, Loss: 0.7838\n",
      "Epoch 34/100, Loss: 0.7611\n",
      "Epoch 35/100, Loss: 0.8019\n",
      "Epoch 36/100, Loss: 0.7673\n",
      "Epoch 37/100, Loss: 0.7336\n",
      "Epoch 38/100, Loss: 0.7387\n",
      "Epoch 39/100, Loss: 0.7337\n",
      "Epoch 40/100, Loss: 0.7023\n",
      "Epoch 41/100, Loss: 0.6928\n",
      "Epoch 42/100, Loss: 0.6998\n",
      "Epoch 43/100, Loss: 0.6709\n",
      "Epoch 44/100, Loss: 0.6773\n",
      "Epoch 45/100, Loss: 0.6567\n",
      "Epoch 46/100, Loss: 0.6646\n",
      "Epoch 47/100, Loss: 0.7028\n",
      "Epoch 48/100, Loss: 0.6727\n",
      "Epoch 49/100, Loss: 0.6658\n",
      "Epoch 50/100, Loss: 0.6700\n",
      "Epoch 51/100, Loss: 0.6793\n",
      "Epoch 52/100, Loss: 0.6517\n",
      "Epoch 53/100, Loss: 0.6557\n",
      "Epoch 54/100, Loss: 0.6509\n",
      "Epoch 55/100, Loss: 0.6519\n",
      "Epoch 56/100, Loss: 0.6445\n",
      "Epoch 57/100, Loss: 0.6812\n",
      "Epoch 58/100, Loss: 0.6337\n",
      "Epoch 59/100, Loss: 0.6523\n",
      "Epoch 60/100, Loss: 0.6183\n",
      "Epoch 61/100, Loss: 0.6294\n",
      "Epoch 62/100, Loss: 0.6191\n",
      "Epoch 63/100, Loss: 0.6279\n",
      "Epoch 64/100, Loss: 0.6161\n",
      "Epoch 65/100, Loss: 0.6391\n",
      "Epoch 66/100, Loss: 0.6225\n",
      "Epoch 67/100, Loss: 0.6248\n",
      "Epoch 68/100, Loss: 0.6475\n",
      "Epoch 69/100, Loss: 0.6109\n",
      "Epoch 70/100, Loss: 0.5941\n",
      "Epoch 71/100, Loss: 0.6201\n",
      "Epoch 72/100, Loss: 0.5956\n",
      "Epoch 73/100, Loss: 0.6529\n",
      "Epoch 74/100, Loss: 0.6322\n",
      "Epoch 75/100, Loss: 0.6328\n",
      "Epoch 76/100, Loss: 0.6041\n",
      "Epoch 77/100, Loss: 0.6295\n",
      "Epoch 78/100, Loss: 0.6428\n",
      "Epoch 79/100, Loss: 0.6440\n",
      "Epoch 80/100, Loss: 0.6566\n",
      "Epoch 81/100, Loss: 0.6173\n",
      "Epoch 82/100, Loss: 0.6038\n",
      "Epoch 83/100, Loss: 0.6325\n",
      "Epoch 84/100, Loss: 0.6016\n",
      "Epoch 85/100, Loss: 0.6395\n",
      "Epoch 86/100, Loss: 0.6150\n",
      "Epoch 87/100, Loss: 0.5962\n",
      "Epoch 88/100, Loss: 0.5871\n",
      "Epoch 89/100, Loss: 0.6124\n",
      "Epoch 90/100, Loss: 0.6050\n",
      "Epoch 91/100, Loss: 0.6033\n",
      "Epoch 92/100, Loss: 0.5908\n",
      "Epoch 93/100, Loss: 0.5944\n",
      "Epoch 94/100, Loss: 0.6185\n",
      "Epoch 95/100, Loss: 0.5908\n",
      "Epoch 96/100, Loss: 0.5841\n",
      "Epoch 97/100, Loss: 0.5883\n",
      "Epoch 98/100, Loss: 0.6334\n",
      "Epoch 99/100, Loss: 0.6170\n",
      "Epoch 100/100, Loss: 0.5965\n",
      "Fold 7 R²: -0.1296, RMSE: 2.2999, MAPE: 0.3566\n",
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.3823\n",
      "Epoch 2/100, Loss: 1.4307\n",
      "Epoch 3/100, Loss: 1.3318\n",
      "Epoch 4/100, Loss: 1.3241\n",
      "Epoch 5/100, Loss: 1.2803\n",
      "Epoch 6/100, Loss: 1.3104\n",
      "Epoch 7/100, Loss: 1.1512\n",
      "Epoch 8/100, Loss: 1.1110\n",
      "Epoch 9/100, Loss: 1.1008\n",
      "Epoch 10/100, Loss: 1.0607\n",
      "Epoch 11/100, Loss: 1.0937\n",
      "Epoch 12/100, Loss: 1.0838\n",
      "Epoch 13/100, Loss: 1.0366\n",
      "Epoch 14/100, Loss: 1.0185\n",
      "Epoch 15/100, Loss: 0.9761\n",
      "Epoch 16/100, Loss: 0.9455\n",
      "Epoch 17/100, Loss: 0.9722\n",
      "Epoch 18/100, Loss: 0.9897\n",
      "Epoch 19/100, Loss: 0.9760\n",
      "Epoch 20/100, Loss: 0.9287\n",
      "Epoch 21/100, Loss: 0.9427\n",
      "Epoch 22/100, Loss: 0.9226\n",
      "Epoch 23/100, Loss: 0.9696\n",
      "Epoch 24/100, Loss: 0.8659\n",
      "Epoch 25/100, Loss: 0.8714\n",
      "Epoch 26/100, Loss: 0.8690\n",
      "Epoch 27/100, Loss: 0.8470\n",
      "Epoch 28/100, Loss: 0.9017\n",
      "Epoch 29/100, Loss: 0.9086\n",
      "Epoch 30/100, Loss: 0.8114\n",
      "Epoch 31/100, Loss: 0.8085\n",
      "Epoch 32/100, Loss: 0.8107\n",
      "Epoch 33/100, Loss: 0.7756\n",
      "Epoch 34/100, Loss: 0.7844\n",
      "Epoch 35/100, Loss: 0.7825\n",
      "Epoch 36/100, Loss: 0.8017\n",
      "Epoch 37/100, Loss: 0.7341\n",
      "Epoch 38/100, Loss: 0.7859\n",
      "Epoch 39/100, Loss: 0.7413\n",
      "Epoch 40/100, Loss: 0.7469\n",
      "Epoch 41/100, Loss: 0.7231\n",
      "Epoch 42/100, Loss: 0.7593\n",
      "Epoch 43/100, Loss: 0.7457\n",
      "Epoch 44/100, Loss: 0.7224\n",
      "Epoch 45/100, Loss: 0.7599\n",
      "Epoch 46/100, Loss: 0.7234\n",
      "Epoch 47/100, Loss: 0.7226\n",
      "Epoch 48/100, Loss: 0.7545\n",
      "Epoch 49/100, Loss: 0.7166\n",
      "Epoch 50/100, Loss: 0.7091\n",
      "Epoch 51/100, Loss: 0.7496\n",
      "Epoch 52/100, Loss: 0.7136\n",
      "Epoch 53/100, Loss: 0.6903\n",
      "Epoch 54/100, Loss: 0.7511\n",
      "Epoch 55/100, Loss: 0.6929\n",
      "Epoch 56/100, Loss: 0.6875\n",
      "Epoch 57/100, Loss: 0.6625\n",
      "Epoch 58/100, Loss: 0.7009\n",
      "Epoch 59/100, Loss: 0.6655\n",
      "Epoch 60/100, Loss: 0.6894\n",
      "Epoch 61/100, Loss: 0.6688\n",
      "Epoch 62/100, Loss: 0.6903\n",
      "Epoch 63/100, Loss: 0.6771\n",
      "Epoch 64/100, Loss: 0.7016\n",
      "Epoch 65/100, Loss: 0.7247\n",
      "Epoch 66/100, Loss: 0.6746\n",
      "Epoch 67/100, Loss: 0.6572\n",
      "Epoch 68/100, Loss: 0.7012\n",
      "Epoch 69/100, Loss: 0.6690\n",
      "Epoch 70/100, Loss: 0.6713\n",
      "Epoch 71/100, Loss: 0.6771\n",
      "Epoch 72/100, Loss: 0.6829\n",
      "Epoch 73/100, Loss: 0.6295\n",
      "Epoch 74/100, Loss: 0.6544\n",
      "Epoch 75/100, Loss: 0.6710\n",
      "Epoch 76/100, Loss: 0.6552\n",
      "Epoch 77/100, Loss: 0.6345\n",
      "Epoch 78/100, Loss: 0.6473\n",
      "Epoch 79/100, Loss: 0.6516\n",
      "Epoch 80/100, Loss: 0.6491\n",
      "Epoch 81/100, Loss: 0.6425\n",
      "Epoch 82/100, Loss: 0.6463\n",
      "Epoch 83/100, Loss: 0.6404\n",
      "Epoch 84/100, Loss: 0.6405\n",
      "Epoch 85/100, Loss: 0.6497\n",
      "Epoch 86/100, Loss: 0.6528\n",
      "Epoch 87/100, Loss: 0.6234\n",
      "Epoch 88/100, Loss: 0.6155\n",
      "Epoch 89/100, Loss: 0.6161\n",
      "Epoch 90/100, Loss: 0.6572\n",
      "Epoch 91/100, Loss: 0.6713\n",
      "Epoch 92/100, Loss: 0.6483\n",
      "Epoch 93/100, Loss: 0.6383\n",
      "Epoch 94/100, Loss: 0.6327\n",
      "Epoch 95/100, Loss: 0.6374\n",
      "Epoch 96/100, Loss: 0.6374\n",
      "Epoch 97/100, Loss: 0.6378\n",
      "Epoch 98/100, Loss: 0.6103\n",
      "Epoch 99/100, Loss: 0.6444\n",
      "Epoch 100/100, Loss: 0.6297\n",
      "Fold 8 R²: -0.0171, RMSE: 2.0651, MAPE: 0.3307\n",
      "Fold 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.3042\n",
      "Epoch 2/100, Loss: 1.4264\n",
      "Epoch 3/100, Loss: 1.3343\n",
      "Epoch 4/100, Loss: 1.3033\n",
      "Epoch 5/100, Loss: 1.2016\n",
      "Epoch 6/100, Loss: 1.2189\n",
      "Epoch 7/100, Loss: 1.1310\n",
      "Epoch 8/100, Loss: 1.0686\n",
      "Epoch 9/100, Loss: 1.0367\n",
      "Epoch 10/100, Loss: 1.0053\n",
      "Epoch 11/100, Loss: 1.0487\n",
      "Epoch 12/100, Loss: 1.0417\n",
      "Epoch 13/100, Loss: 1.0098\n",
      "Epoch 14/100, Loss: 0.9793\n",
      "Epoch 15/100, Loss: 0.9354\n",
      "Epoch 16/100, Loss: 0.9053\n",
      "Epoch 17/100, Loss: 0.9239\n",
      "Epoch 18/100, Loss: 0.9449\n",
      "Epoch 19/100, Loss: 0.9195\n",
      "Epoch 20/100, Loss: 0.8794\n",
      "Epoch 21/100, Loss: 0.8677\n",
      "Epoch 22/100, Loss: 0.8415\n",
      "Epoch 23/100, Loss: 0.8462\n",
      "Epoch 24/100, Loss: 0.8349\n",
      "Epoch 25/100, Loss: 0.8034\n",
      "Epoch 26/100, Loss: 0.7885\n",
      "Epoch 27/100, Loss: 0.7867\n",
      "Epoch 28/100, Loss: 0.8057\n",
      "Epoch 29/100, Loss: 0.7867\n",
      "Epoch 30/100, Loss: 0.7742\n",
      "Epoch 31/100, Loss: 0.7439\n",
      "Epoch 32/100, Loss: 0.7267\n",
      "Epoch 33/100, Loss: 0.7441\n",
      "Epoch 34/100, Loss: 0.7349\n",
      "Epoch 35/100, Loss: 0.7094\n",
      "Epoch 36/100, Loss: 0.7067\n",
      "Epoch 37/100, Loss: 0.7248\n",
      "Epoch 38/100, Loss: 0.7179\n",
      "Epoch 39/100, Loss: 0.7757\n",
      "Epoch 40/100, Loss: 0.6703\n",
      "Epoch 41/100, Loss: 0.6964\n",
      "Epoch 42/100, Loss: 0.7073\n",
      "Epoch 43/100, Loss: 0.6610\n",
      "Epoch 44/100, Loss: 0.6664\n",
      "Epoch 45/100, Loss: 0.6740\n",
      "Epoch 46/100, Loss: 0.6832\n",
      "Epoch 47/100, Loss: 0.6460\n",
      "Epoch 48/100, Loss: 0.6272\n",
      "Epoch 49/100, Loss: 0.6093\n",
      "Epoch 50/100, Loss: 0.6320\n",
      "Epoch 51/100, Loss: 0.6474\n",
      "Epoch 52/100, Loss: 0.6334\n",
      "Epoch 53/100, Loss: 0.6167\n",
      "Epoch 54/100, Loss: 0.6103\n",
      "Epoch 55/100, Loss: 0.6445\n",
      "Epoch 56/100, Loss: 0.6709\n",
      "Epoch 57/100, Loss: 0.6187\n",
      "Epoch 58/100, Loss: 0.6202\n",
      "Epoch 59/100, Loss: 0.6062\n",
      "Epoch 60/100, Loss: 0.6050\n",
      "Epoch 61/100, Loss: 0.6444\n",
      "Epoch 62/100, Loss: 0.7270\n",
      "Epoch 63/100, Loss: 0.6357\n",
      "Epoch 64/100, Loss: 0.6179\n",
      "Epoch 65/100, Loss: 0.6048\n",
      "Epoch 66/100, Loss: 0.6473\n",
      "Epoch 67/100, Loss: 0.6422\n",
      "Epoch 68/100, Loss: 0.5948\n",
      "Epoch 69/100, Loss: 0.6170\n",
      "Epoch 70/100, Loss: 0.6123\n",
      "Epoch 71/100, Loss: 0.6081\n",
      "Epoch 72/100, Loss: 0.6192\n",
      "Epoch 73/100, Loss: 0.6066\n",
      "Epoch 74/100, Loss: 0.6005\n",
      "Epoch 75/100, Loss: 0.5913\n",
      "Epoch 76/100, Loss: 0.5942\n",
      "Epoch 77/100, Loss: 0.5772\n",
      "Epoch 78/100, Loss: 0.5972\n",
      "Epoch 79/100, Loss: 0.6001\n",
      "Epoch 80/100, Loss: 0.5750\n",
      "Epoch 81/100, Loss: 0.5681\n",
      "Epoch 82/100, Loss: 0.5818\n",
      "Epoch 83/100, Loss: 0.5759\n",
      "Epoch 84/100, Loss: 0.5926\n",
      "Epoch 85/100, Loss: 0.6014\n",
      "Epoch 86/100, Loss: 0.5822\n",
      "Epoch 87/100, Loss: 0.5770\n",
      "Epoch 88/100, Loss: 0.5921\n",
      "Epoch 89/100, Loss: 0.5733\n",
      "Epoch 90/100, Loss: 0.5898\n",
      "Epoch 91/100, Loss: 0.5756\n",
      "Epoch 92/100, Loss: 0.5646\n",
      "Epoch 93/100, Loss: 0.5584\n",
      "Epoch 94/100, Loss: 0.5768\n",
      "Epoch 95/100, Loss: 0.5726\n",
      "Epoch 96/100, Loss: 0.5922\n",
      "Epoch 97/100, Loss: 0.5586\n",
      "Epoch 98/100, Loss: 0.5966\n",
      "Epoch 99/100, Loss: 0.6486\n",
      "Epoch 100/100, Loss: 0.6278\n",
      "Fold 9 R²: -0.0744, RMSE: 2.4800, MAPE: 673609852911616.0000\n",
      "Fold 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.0255\n",
      "Epoch 2/100, Loss: 1.4648\n",
      "Epoch 3/100, Loss: 1.4307\n",
      "Epoch 4/100, Loss: 1.3105\n",
      "Epoch 5/100, Loss: 1.2428\n",
      "Epoch 6/100, Loss: 1.2070\n",
      "Epoch 7/100, Loss: 1.1525\n",
      "Epoch 8/100, Loss: 1.1696\n",
      "Epoch 9/100, Loss: 1.1736\n",
      "Epoch 10/100, Loss: 1.0861\n",
      "Epoch 11/100, Loss: 1.0716\n",
      "Epoch 12/100, Loss: 1.1237\n",
      "Epoch 13/100, Loss: 1.0177\n",
      "Epoch 14/100, Loss: 1.0042\n",
      "Epoch 15/100, Loss: 1.0506\n",
      "Epoch 16/100, Loss: 0.9488\n",
      "Epoch 17/100, Loss: 0.9988\n",
      "Epoch 18/100, Loss: 0.9708\n",
      "Epoch 19/100, Loss: 0.9061\n",
      "Epoch 20/100, Loss: 0.9115\n",
      "Epoch 21/100, Loss: 0.9435\n",
      "Epoch 22/100, Loss: 0.8599\n",
      "Epoch 23/100, Loss: 0.8491\n",
      "Epoch 24/100, Loss: 0.8346\n",
      "Epoch 25/100, Loss: 0.8681\n",
      "Epoch 26/100, Loss: 0.8524\n",
      "Epoch 27/100, Loss: 0.8489\n",
      "Epoch 28/100, Loss: 0.8359\n",
      "Epoch 29/100, Loss: 0.8177\n",
      "Epoch 30/100, Loss: 0.7655\n",
      "Epoch 31/100, Loss: 0.7906\n",
      "Epoch 32/100, Loss: 0.8032\n",
      "Epoch 33/100, Loss: 0.7784\n",
      "Epoch 34/100, Loss: 0.7766\n",
      "Epoch 35/100, Loss: 0.7177\n",
      "Epoch 36/100, Loss: 0.7370\n",
      "Epoch 37/100, Loss: 0.7288\n",
      "Epoch 38/100, Loss: 0.7535\n",
      "Epoch 39/100, Loss: 0.7236\n",
      "Epoch 40/100, Loss: 0.7383\n",
      "Epoch 41/100, Loss: 0.7355\n",
      "Epoch 42/100, Loss: 0.7618\n",
      "Epoch 43/100, Loss: 0.7562\n",
      "Epoch 44/100, Loss: 0.7095\n",
      "Epoch 45/100, Loss: 0.6955\n",
      "Epoch 46/100, Loss: 0.6861\n",
      "Epoch 47/100, Loss: 0.7152\n",
      "Epoch 48/100, Loss: 0.6907\n",
      "Epoch 49/100, Loss: 0.7027\n",
      "Epoch 50/100, Loss: 0.6689\n",
      "Epoch 51/100, Loss: 0.6616\n",
      "Epoch 52/100, Loss: 0.6761\n",
      "Epoch 53/100, Loss: 0.6588\n",
      "Epoch 54/100, Loss: 0.6632\n",
      "Epoch 55/100, Loss: 0.6545\n",
      "Epoch 56/100, Loss: 0.6865\n",
      "Epoch 57/100, Loss: 0.7512\n",
      "Epoch 58/100, Loss: 0.7245\n",
      "Epoch 59/100, Loss: 0.7057\n",
      "Epoch 60/100, Loss: 0.6620\n",
      "Epoch 61/100, Loss: 0.6435\n",
      "Epoch 62/100, Loss: 0.6423\n",
      "Epoch 63/100, Loss: 0.6773\n",
      "Epoch 64/100, Loss: 0.6692\n",
      "Epoch 65/100, Loss: 0.6539\n",
      "Epoch 66/100, Loss: 0.6647\n",
      "Epoch 67/100, Loss: 0.6430\n",
      "Epoch 68/100, Loss: 0.6564\n",
      "Epoch 69/100, Loss: 0.6625\n",
      "Epoch 70/100, Loss: 0.6129\n",
      "Epoch 71/100, Loss: 0.6284\n",
      "Epoch 72/100, Loss: 0.6548\n",
      "Epoch 73/100, Loss: 0.6622\n",
      "Epoch 74/100, Loss: 0.6332\n",
      "Epoch 75/100, Loss: 0.6140\n",
      "Epoch 76/100, Loss: 0.6559\n",
      "Epoch 77/100, Loss: 0.6612\n",
      "Epoch 78/100, Loss: 0.6521\n",
      "Epoch 79/100, Loss: 0.6177\n",
      "Epoch 80/100, Loss: 0.6449\n",
      "Epoch 81/100, Loss: 0.6378\n",
      "Epoch 82/100, Loss: 0.6469\n",
      "Epoch 83/100, Loss: 0.6449\n",
      "Epoch 84/100, Loss: 0.6177\n",
      "Epoch 85/100, Loss: 0.6171\n",
      "Epoch 86/100, Loss: 0.6129\n",
      "Epoch 87/100, Loss: 0.6514\n",
      "Epoch 88/100, Loss: 0.6278\n",
      "Epoch 89/100, Loss: 0.5988\n",
      "Epoch 90/100, Loss: 0.6078\n",
      "Epoch 91/100, Loss: 0.6071\n",
      "Epoch 92/100, Loss: 0.6011\n",
      "Epoch 93/100, Loss: 0.6069\n",
      "Epoch 94/100, Loss: 0.6039\n",
      "Epoch 95/100, Loss: 0.5836\n",
      "Epoch 96/100, Loss: 0.5968\n",
      "Epoch 97/100, Loss: 0.5878\n",
      "Epoch 98/100, Loss: 0.6060\n",
      "Epoch 99/100, Loss: 0.6083\n",
      "Epoch 100/100, Loss: 0.6574\n",
      "Fold 10 R²: -0.0960, RMSE: 2.4204, MAPE: 227941413814272.0000\n",
      "Predictions saved to 'data_with_predictions.csv'!\n",
      "Final Metrics - R²: 0.0250, RMSE: 2.2568, MAPE: 121773666101835.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load embeddings\n",
    "# combined_batches = torch.load('combined_batches.pt', map_location=device)  # Shape: [973, 5, 512, 768]\n",
    "combined_embeddings_ = combined_tensor[:, :, 0, :]  # Extract CLS token (dim=2), Shape: [973, 5, 768]\n",
    "cls_embeddings = combined_embeddings_.mean(dim=1)  # Mean pooling across 5 batches, Shape: [973, 768]\n",
    "print(f\"Embeddings loaded. Shape: {cls_embeddings.shape}\")\n",
    "\n",
    "labels = torch.tensor(data['Total_Marks']).float()  # Assuming 'Total_Marks' column contains the target labels\n",
    "assert len(labels) == cls_embeddings.size(0), \"Number of labels must match number of embeddings\"\n",
    "\n",
    "# Normalize labels (min-max normalization)\n",
    "labels_min = labels.min()\n",
    "labels_max = labels.max()\n",
    "labels = (labels - labels_min) / (labels_max - labels_min)  # Normalize to range [0, 1]\n",
    "\n",
    "# Custom Dataset for embeddings and labels\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# BiLSTM Model for regression with an extra hidden layer\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 128)  # Additional hidden layer\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)  # [batch_size, seq_len, 2*hidden_dim]\n",
    "        lstm_out = lstm_out.mean(dim=1)  # Mean pooling over sequence length\n",
    "        fc1_out = torch.relu(self.fc1(lstm_out))  # First hidden layer\n",
    "        output = self.fc2(fc1_out)  # Output layer\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = cls_embeddings.size(-1)  # 768\n",
    "hidden_dim = 256  # Hidden state size\n",
    "num_layers = 2   # Number of BiLSTM layers\n",
    "batch_size = 32  # Batch size for training\n",
    "num_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 10-Fold Cross-Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "predicted_values = np.zeros(len(labels))  # Store predictions for all folds\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(cls_embeddings)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_dataset = CodeDataset(cls_embeddings[train_idx], labels[train_idx])\n",
    "    test_dataset = CodeDataset(cls_embeddings[test_idx], labels[test_idx])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Reset model and optimizer\n",
    "    model = BiLSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for inputs, target in train_loader:\n",
    "            inputs = inputs.to(device).float().unsqueeze(1)  # Add sequence length dimension\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs).squeeze(-1)  # [batch_size]\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs = inputs.to(device).float().unsqueeze(1)\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            output = model(inputs).squeeze(-1)  # [batch_size]\n",
    "            test_preds.extend(output.cpu().numpy())\n",
    "            test_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # De-normalize predictions and targets\n",
    "    test_preds = np.array(test_preds) * (labels_max - labels_min).item() + labels_min.item()\n",
    "    test_targets = np.array(test_targets) * (labels_max - labels_min).item() + labels_min.item()\n",
    "\n",
    "    # Store predictions back into the dataset\n",
    "    data.loc[test_idx, 'Predictions'] = test_preds\n",
    "\n",
    "    # Store predictions in their respective indices for metrics calculation\n",
    "    predicted_values[test_idx] = test_preds\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(test_targets, test_preds)\n",
    "    rmse = mean_squared_error(test_targets, test_preds, squared=False)\n",
    "    mape = mean_absolute_percentage_error(test_targets, test_preds)\n",
    "\n",
    "    print(f\"Fold {fold + 1} R²: {r2:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "\n",
    "# Save the predicted values back to 'data'\n",
    "data.to_csv('data_with_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'data_with_predictions.csv'!\")\n",
    "\n",
    "# Final metrics on the entire dataset\n",
    "r2_final = r2_score(labels * (labels_max - labels_min).item() + labels_min.item(), predicted_values)\n",
    "rmse_final = mean_squared_error(labels * (labels_max - labels_min).item() + labels_min.item(), predicted_values, squared=False)\n",
    "mape_final = mean_absolute_percentage_error(labels * (labels_max - labels_min).item() + labels_min.item(), predicted_values)\n",
    "\n",
    "print(f\"Final Metrics - R²: {r2_final:.4f}, RMSE: {rmse_final:.4f}, MAPE: {mape_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bfcf41a-c4fb-4184-ad85-28bca64ee1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.941752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>5.128729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>4.312079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.914923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.299567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  Predictions  \n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     6.941752  \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     5.128729  \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     4.312079  \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     6.914923  \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     7.299567  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61b47a2f-a704-41a0-9691-d1046404c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.2363955809803833"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(data['Total_Marks'], data['Predictions'],squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec21ba-91ff-4b24-8fcb-7cbc9dfcb255",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21defe70-d3bf-4fa6-a69b-9b51147c908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM output shape: torch.Size([973, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Define BiLSTM model\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)  # Output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        return lstm_out\n",
    "\n",
    "# Parameters\n",
    "input_dim = cls_tokens.shape[-1]  # 768\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize and process through BiLSTM\n",
    "bilstm_model = BiLSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "bilstm_out = bilstm_model(cls_tokens)  # Input shape: [100, 5, 768]\n",
    "\n",
    "print(f\"BiLSTM output shape: {bilstm_out.shape}\")  # Expected: [100, 5, 256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa1cabd-e6b3-4992-81b2-02ee213a680e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.9638e-02,  4.3596e-02,  3.7416e-02,  ..., -1.6467e-03,\n",
      "          -1.8959e-02,  1.8841e-02],\n",
      "         [-7.0362e-02,  1.1500e-01,  4.4823e-02,  ...,  2.4961e-03,\n",
      "          -1.3251e-02,  1.6140e-02],\n",
      "         [-6.7673e-02,  1.0427e-01,  5.5073e-02,  ..., -2.8472e-02,\n",
      "          -2.1572e-02,  6.8984e-03],\n",
      "         [-7.2155e-02,  8.8017e-02,  6.2113e-02,  ..., -3.0757e-02,\n",
      "          -1.8411e-02,  1.1647e-02],\n",
      "         [-7.2060e-02,  7.2969e-02,  7.0614e-02,  ..., -2.6238e-02,\n",
      "          -1.4617e-02,  1.3848e-02]],\n",
      "\n",
      "        [[-8.8478e-02,  2.6850e-02,  5.0853e-02,  ..., -8.5485e-02,\n",
      "           9.4792e-03,  6.5808e-03],\n",
      "         [-9.0818e-02,  3.1558e-02,  6.3055e-02,  ..., -6.6047e-02,\n",
      "           3.0052e-02, -8.8734e-03],\n",
      "         [-8.1569e-02,  4.0304e-02,  6.8501e-02,  ..., -2.4639e-02,\n",
      "           8.2662e-03,  5.4836e-04],\n",
      "         [-7.6553e-02,  4.6134e-02,  6.8535e-02,  ..., -3.0234e-02,\n",
      "           6.9513e-05,  9.3080e-03],\n",
      "         [-7.1653e-02,  4.6879e-02,  7.1848e-02,  ..., -2.6516e-02,\n",
      "          -5.7914e-03,  1.2966e-02]],\n",
      "\n",
      "        [[-9.1712e-02,  3.1809e-02,  3.7873e-02,  ..., -1.6864e-02,\n",
      "          -3.6918e-02, -4.6498e-03],\n",
      "         [-1.1554e-01,  4.9909e-02,  3.9478e-02,  ..., -8.4467e-03,\n",
      "          -2.9888e-02, -2.0042e-02],\n",
      "         [-8.8840e-02,  4.8097e-02,  4.7201e-02,  ..., -1.9574e-02,\n",
      "          -2.2265e-02, -8.3039e-03],\n",
      "         [-7.9772e-02,  4.8387e-02,  5.0807e-02,  ..., -2.8291e-02,\n",
      "          -1.7861e-02,  4.2363e-03],\n",
      "         [-7.2521e-02,  4.7287e-02,  5.8121e-02,  ..., -2.6102e-02,\n",
      "          -1.3957e-02,  1.0781e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.4888e-02,  2.7756e-02,  2.9379e-02,  ..., -3.8450e-02,\n",
      "          -3.4719e-03, -9.2318e-04],\n",
      "         [-6.1754e-02,  4.3699e-02,  5.1069e-02,  ..., -2.9332e-02,\n",
      "          -4.3706e-03,  3.5983e-03],\n",
      "         [-6.9404e-02,  4.8345e-02,  5.9884e-02,  ..., -3.1071e-02,\n",
      "          -7.4052e-03,  6.7882e-03],\n",
      "         [-7.2419e-02,  4.8523e-02,  6.4158e-02,  ..., -3.2518e-02,\n",
      "          -1.0313e-02,  1.1377e-02],\n",
      "         [-7.0895e-02,  4.6132e-02,  6.9987e-02,  ..., -2.7115e-02,\n",
      "          -1.0877e-02,  1.3488e-02]],\n",
      "\n",
      "        [[-9.5013e-02,  2.1692e-02,  3.2402e-02,  ..., -2.4400e-02,\n",
      "          -1.5948e-02,  1.2178e-02],\n",
      "         [-1.0774e-01,  5.3348e-02,  4.5821e-02,  ..., -2.7745e-02,\n",
      "          -1.2407e-02,  1.1747e-02],\n",
      "         [-9.6478e-02,  5.6240e-02,  5.5060e-02,  ..., -3.2324e-02,\n",
      "          -9.4473e-03,  4.9268e-03],\n",
      "         [-8.9097e-02,  5.3207e-02,  5.9396e-02,  ..., -3.3480e-02,\n",
      "          -1.0866e-02,  1.0285e-02],\n",
      "         [-8.1264e-02,  4.8826e-02,  6.5833e-02,  ..., -2.7780e-02,\n",
      "          -1.0881e-02,  1.3097e-02]],\n",
      "\n",
      "        [[-9.0814e-02,  3.5724e-02,  4.0183e-02,  ...,  1.6895e-02,\n",
      "          -5.0345e-02,  1.1570e-02],\n",
      "         [-9.8581e-02,  7.3806e-02,  8.9741e-02,  ...,  3.5500e-02,\n",
      "          -4.5015e-02, -4.3210e-03],\n",
      "         [-8.0311e-02,  6.5937e-02,  8.8581e-02,  ..., -1.5950e-02,\n",
      "          -1.9829e-02,  5.5063e-03],\n",
      "         [-7.3802e-02,  6.3750e-02,  8.1394e-02,  ..., -2.8455e-02,\n",
      "          -1.8330e-02,  1.2316e-02],\n",
      "         [-6.9282e-02,  5.9316e-02,  7.9988e-02,  ..., -2.6900e-02,\n",
      "          -1.4996e-02,  1.4382e-02]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(bilstm_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c94024-1980-4877-8580-087852cfa462",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d982f99-1130-4ae4-9945-dcc44152c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest model...\n",
      "\n",
      "Processing Fold 1...\n",
      "Fold 1 Results: R² = 0.3483, RMSE = 1.9051, MAPE = 123374436029899.4688\n",
      "\n",
      "Processing Fold 2...\n",
      "Fold 2 Results: R² = 0.1843, RMSE = 2.1504, MAPE = 0.4048\n",
      "\n",
      "Processing Fold 3...\n",
      "Fold 3 Results: R² = 0.2738, RMSE = 2.0334, MAPE = 0.4206\n",
      "\n",
      "Processing Fold 4...\n",
      "Fold 4 Results: R² = 0.0673, RMSE = 2.0666, MAPE = 0.3431\n",
      "\n",
      "Processing Fold 5...\n",
      "Fold 5 Results: R² = 0.2262, RMSE = 1.9157, MAPE = 191774415060349.1250\n",
      "\n",
      "Processing Fold 6...\n",
      "Fold 6 Results: R² = 0.3519, RMSE = 1.8520, MAPE = 0.4411\n",
      "\n",
      "Processing Fold 7...\n",
      "Fold 7 Results: R² = 0.1583, RMSE = 1.9853, MAPE = 0.3227\n",
      "\n",
      "Processing Fold 8...\n",
      "Fold 8 Results: R² = 0.1970, RMSE = 1.8349, MAPE = 0.3054\n",
      "\n",
      "Processing Fold 9...\n",
      "Fold 9 Results: R² = 0.2675, RMSE = 2.0477, MAPE = 680527730438096.8750\n",
      "\n",
      "Processing Fold 10...\n",
      "Fold 10 Results: R² = 0.0443, RMSE = 2.2601, MAPE = 223694810504663.7500\n",
      "\n",
      "Random Forest - Average Results: R² = 0.2119, RMSE = 2.0051, MAPE = 121937139203301.1562\n",
      "\n",
      "Training XGBoost model...\n",
      "\n",
      "Processing Fold 1...\n",
      "Fold 1 Results: R² = 0.2726, RMSE = 2.0128, MAPE = 122589739525935.3438\n",
      "\n",
      "Processing Fold 2...\n",
      "Fold 2 Results: R² = 0.0405, RMSE = 2.3324, MAPE = 0.3985\n",
      "\n",
      "Processing Fold 3...\n",
      "Fold 3 Results: R² = 0.1999, RMSE = 2.1345, MAPE = 0.4007\n",
      "\n",
      "Processing Fold 4...\n",
      "Fold 4 Results: R² = -0.0342, RMSE = 2.1761, MAPE = 0.3547\n",
      "\n",
      "Processing Fold 5...\n",
      "Fold 5 Results: R² = 0.0837, RMSE = 2.0845, MAPE = 186339591427695.2188\n",
      "\n",
      "Processing Fold 6...\n",
      "Fold 6 Results: R² = 0.2294, RMSE = 2.0194, MAPE = 0.4756\n",
      "\n",
      "Processing Fold 7...\n",
      "Fold 7 Results: R² = 0.0821, RMSE = 2.0733, MAPE = 0.3360\n",
      "\n",
      "Processing Fold 8...\n",
      "Fold 8 Results: R² = 0.0517, RMSE = 1.9939, MAPE = 0.3158\n",
      "\n",
      "Processing Fold 9...\n",
      "Fold 9 Results: R² = 0.2166, RMSE = 2.1177, MAPE = 709855059091709.6250\n",
      "\n",
      "Processing Fold 10...\n",
      "Fold 10 Results: R² = -0.0936, RMSE = 2.4178, MAPE = 252221378843426.8438\n",
      "\n",
      "XGBoost - Average Results: R² = 0.1049, RMSE = 2.1362, MAPE = 127100576888876.9219\n",
      "\n",
      "Training CatBoost model...\n",
      "\n",
      "Processing Fold 1...\n",
      "Fold 1 Results: R² = 0.3179, RMSE = 1.9491, MAPE = 122591178405058.0000\n",
      "\n",
      "Processing Fold 2...\n",
      "Fold 2 Results: R² = 0.1460, RMSE = 2.2004, MAPE = 0.3575\n",
      "\n",
      "Processing Fold 3...\n",
      "Fold 3 Results: R² = 0.1906, RMSE = 2.1468, MAPE = 0.4069\n",
      "\n",
      "Processing Fold 4...\n",
      "Fold 4 Results: R² = -0.0025, RMSE = 2.1425, MAPE = 0.3549\n",
      "\n",
      "Processing Fold 5...\n",
      "Fold 5 Results: R² = 0.1506, RMSE = 2.0070, MAPE = 189186675970994.6250\n",
      "\n",
      "Processing Fold 6...\n",
      "Fold 6 Results: R² = 0.3234, RMSE = 1.8923, MAPE = 0.4511\n",
      "\n",
      "Processing Fold 7...\n",
      "Fold 7 Results: R² = 0.1540, RMSE = 1.9904, MAPE = 0.3249\n",
      "\n",
      "Processing Fold 8...\n",
      "Fold 8 Results: R² = 0.0950, RMSE = 1.9478, MAPE = 0.3163\n",
      "\n",
      "Processing Fold 9...\n",
      "Fold 9 Results: R² = 0.2426, RMSE = 2.0823, MAPE = 647952340168748.2500\n",
      "\n",
      "Processing Fold 10...\n",
      "Fold 10 Results: R² = 0.0378, RMSE = 2.2678, MAPE = 221641201220208.7812\n",
      "\n",
      "CatBoost - Average Results: R² = 0.1655, RMSE = 2.0626, MAPE = 118137139576501.2031\n",
      "\n",
      "Training SVR model...\n",
      "\n",
      "Processing Fold 1...\n",
      "Fold 1 Results: R² = 0.1161, RMSE = 2.2188, MAPE = 279107687391292.2188\n",
      "\n",
      "Processing Fold 2...\n",
      "Fold 2 Results: R² = 0.0854, RMSE = 2.2771, MAPE = 0.5477\n",
      "\n",
      "Processing Fold 3...\n",
      "Fold 3 Results: R² = 0.1421, RMSE = 2.2102, MAPE = 0.5396\n",
      "\n",
      "Processing Fold 4...\n",
      "Fold 4 Results: R² = -0.0148, RMSE = 2.1556, MAPE = 0.3766\n",
      "\n",
      "Processing Fold 5...\n",
      "Fold 5 Results: R² = 0.0528, RMSE = 2.1194, MAPE = 283100565425570.6875\n",
      "\n",
      "Processing Fold 6...\n",
      "Fold 6 Results: R² = 0.0488, RMSE = 2.2437, MAPE = 0.5808\n",
      "\n",
      "Processing Fold 7...\n",
      "Fold 7 Results: R² = 0.0496, RMSE = 2.1096, MAPE = 0.3768\n",
      "\n",
      "Processing Fold 8...\n",
      "Fold 8 Results: R² = 0.0978, RMSE = 1.9449, MAPE = 0.3881\n",
      "\n",
      "Processing Fold 9...\n",
      "Fold 9 Results: R² = -0.0017, RMSE = 2.3946, MAPE = 849514278125235.2500\n",
      "\n",
      "Processing Fold 10...\n",
      "Fold 10 Results: R² = 0.0345, RMSE = 2.2718, MAPE = 283124641486706.8750\n",
      "\n",
      "SVR - Average Results: R² = 0.0610, RMSE = 2.1946, MAPE = 169484717242880.8125\n",
      "\n",
      "Training KNN model...\n",
      "\n",
      "Processing Fold 1...\n",
      "Fold 1 Results: R² = 0.2141, RMSE = 2.0922, MAPE = 119483255420033.9531\n",
      "\n",
      "Processing Fold 2...\n",
      "Fold 2 Results: R² = 0.0236, RMSE = 2.3527, MAPE = 0.4842\n",
      "\n",
      "Processing Fold 3...\n",
      "Fold 3 Results: R² = 0.2119, RMSE = 2.1184, MAPE = 0.4508\n",
      "\n",
      "Processing Fold 4...\n",
      "Fold 4 Results: R² = -0.0647, RMSE = 2.2079, MAPE = 0.3673\n",
      "\n",
      "Processing Fold 5...\n",
      "Fold 5 Results: R² = 0.0432, RMSE = 2.1301, MAPE = 120715041558384.8281\n",
      "\n",
      "Processing Fold 6...\n",
      "Fold 6 Results: R² = 0.2646, RMSE = 1.9728, MAPE = 0.4643\n",
      "\n",
      "Processing Fold 7...\n",
      "Fold 7 Results: R² = 0.0646, RMSE = 2.0929, MAPE = 0.3512\n",
      "\n",
      "Processing Fold 8...\n",
      "Fold 8 Results: R² = -0.1176, RMSE = 2.1647, MAPE = 0.3498\n",
      "\n",
      "Processing Fold 9...\n",
      "Fold 9 Results: R² = 0.1707, RMSE = 2.1788, MAPE = 715004476922739.0000\n",
      "\n",
      "Processing Fold 10...\n",
      "Fold 10 Results: R² = -0.1272, RMSE = 2.4547, MAPE = 195001220978929.2812\n",
      "\n",
      "KNN - Average Results: R² = 0.0683, RMSE = 2.1765, MAPE = 115020399488008.9531\n",
      "Evaluation scores saved to AST_model_evaluation_scores.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filtered_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Save predicted values back to the dataset with appropriate column names\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, predictions \u001b[38;5;129;01min\u001b[39;00m predicted_values\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 105\u001b[0m     filtered_data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAST_predicted_values_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Save the dataset with predictions\u001b[39;00m\n\u001b[1;32m    108\u001b[0m predictions_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_with_AST_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Example: Assuming `bilstm_out` is already defined with shape (973, 5, 512)\n",
    "# Flattening the output to (973, 5 * 512)\n",
    "bilstm_out_flattened = bilstm_out.detach().reshape(bilstm_out.size(0), -1).numpy()  # Shape: (973, 2560)\n",
    "# Assuming you have a column `Total_Marks` in your filtered_data for actual values\n",
    "actual_values = data[\"Total_Marks\"]\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=200, max_depth=8, learning_rate=0.1, random_state=42),\n",
    "    \"CatBoost\": CatBoostRegressor(iterations=500, learning_rate=0.1, depth=8, verbose=0),\n",
    "    \"SVR\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Dictionary to store predicted values for each model\n",
    "predicted_values = {\n",
    "    \"Random Forest\": np.zeros(len(actual_values)),\n",
    "    \"XGBoost\": np.zeros(len(actual_values)),\n",
    "    \"CatBoost\": np.zeros(len(actual_values)),\n",
    "    \"SVR\": np.zeros(len(actual_values)),\n",
    "    \"KNN\": np.zeros(len(actual_values))\n",
    "}\n",
    "\n",
    "# Store metrics\n",
    "metrics = {\n",
    "    \"Model\": [],\n",
    "    \"MAPE\": [],\n",
    "    \"R²\": [],\n",
    "    \"RMSE\": [],\n",
    "}\n",
    "\n",
    "# Loop over each model to train and evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    \n",
    "    # Initialize lists to store metrics for each fold\n",
    "    fold_metrics = {\n",
    "        \"R²\": [],\n",
    "        \"RMSE\": [],\n",
    "        \"MAPE\": []\n",
    "    }\n",
    "    \n",
    "    # Perform 10-fold CV\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(bilstm_out_flattened)):\n",
    "        print(f\"\\nProcessing Fold {fold_idx + 1}...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = bilstm_out_flattened[train_idx], bilstm_out_flattened[test_idx]\n",
    "        y_train, y_test = actual_values[train_idx], actual_values[test_idx]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        predicted_values[model_name][test_idx] = y_pred  # Store predictions\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        \n",
    "        # Store fold metrics\n",
    "        fold_metrics[\"R²\"].append(r2)\n",
    "        fold_metrics[\"RMSE\"].append(rmse)\n",
    "        fold_metrics[\"MAPE\"].append(mape)\n",
    "        \n",
    "        print(f\"Fold {fold_idx + 1} Results: R² = {r2:.4f}, RMSE = {rmse:.4f}, MAPE = {mape:.4f}\")\n",
    "\n",
    "    # Calculate average metrics across folds\n",
    "    avg_r2 = np.mean(fold_metrics[\"R²\"])\n",
    "    avg_rmse = np.mean(fold_metrics[\"RMSE\"])\n",
    "    avg_mape = np.mean(fold_metrics[\"MAPE\"])\n",
    "\n",
    "    # Store the model results\n",
    "    metrics[\"Model\"].append(model_name)\n",
    "    metrics[\"MAPE\"].append(avg_mape)\n",
    "    metrics[\"R²\"].append(avg_r2)\n",
    "    metrics[\"RMSE\"].append(avg_rmse)\n",
    "\n",
    "    print(f\"\\n{model_name} - Average Results: R² = {avg_r2:.4f}, RMSE = {avg_rmse:.4f}, MAPE = {avg_mape:.4f}\")\n",
    "\n",
    "# Convert metrics into DataFrame and save it\n",
    "scores_df = pd.DataFrame(metrics)\n",
    "csv_filename = \"AST_model_evaluation_scores.csv\"\n",
    "scores_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Evaluation scores saved to {csv_filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22d72476-eab7-47c2-b22c-11974cfb82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to dataset_with_AST_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predicted values back to the dataset with appropriate column names\n",
    "for model_name, predictions in predicted_values.items():\n",
    "    data[f\"AST_predicted_values_{model_name}\"] = predictions\n",
    "\n",
    "# Save the dataset with predictions\n",
    "predictions_filename = \"dataset_with_AST_predictions.csv\"\n",
    "data.to_csv(predictions_filename, index=False)\n",
    "print(f\"Predictions saved to {predictions_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a183ef-13e1-4793-980c-e037b600d328",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "730d8119-c919-4dec-ae0e-96fe5fc952a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Total_Marks']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c7a4e21-899a-446c-84d5-c7db94e793a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation scores saved to AST_model_evaluation_scores.csv\n",
      "           Model      MAPE        R²      RMSE\n",
      "0  Random Forest  0.379876  0.216205  1.983205\n",
      "1        XGBoost  0.383646  0.109689  2.113669\n",
      "2       CatBoost  0.374203  0.167843  2.043473\n",
      "3            SVR  0.467401  0.077432  2.151619\n",
      "4            KNN  0.416127  0.069700  2.160616\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset if not already loaded\n",
    "# Assuming `filtered_data` is a DataFrame containing the relevant data\n",
    "# filtered_data = pd.read_csv(\"dataset_with_AST_predictions.csv\")\n",
    "\n",
    "# Extract columns for actual and predicted values\n",
    "actual_values = data[\"Total_Marks\"]\n",
    "predicted_columns = {\n",
    "    \"Random Forest\": data[\"AST_predicted_values_Random Forest\"],\n",
    "    \"XGBoost\": data[\"AST_predicted_values_XGBoost\"],\n",
    "    \"CatBoost\": data[\"AST_predicted_values_CatBoost\"],\n",
    "    \"SVR\": data[\"AST_predicted_values_SVR\"],\n",
    "    \"KNN\": data[\"AST_predicted_values_KNN\"],\n",
    "}\n",
    "\n",
    "# Initialize lists to store scores\n",
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"MAPE\": [],\n",
    "    \"R²\": [],\n",
    "    \"RMSE\": [],\n",
    "}\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for model_name, predictions in predicted_columns.items():\n",
    "    mape = mean_absolute_percentage_error(actual_values, predictions)\n",
    "    r2 = r2_score(actual_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_values, predictions))\n",
    "    \n",
    "    # Append results\n",
    "    results[\"Model\"].append(model_name)\n",
    "    results[\"MAPE\"].append(mape)\n",
    "    results[\"R²\"].append(r2)\n",
    "    results[\"RMSE\"].append(rmse)\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "scores_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = \"AST_model_evaluation_scores.csv\"\n",
    "\n",
    "scores_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Evaluation scores saved to {csv_filename}\")\n",
    "\n",
    "# Print the results DataFrame for reference\n",
    "print(scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3154b418-cf19-40f2-b603-49c540bddb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "      <th>AST_predicted_values_Random Forest</th>\n",
       "      <th>AST_predicted_values_XGBoost</th>\n",
       "      <th>AST_predicted_values_CatBoost</th>\n",
       "      <th>AST_predicted_values_SVR</th>\n",
       "      <th>AST_predicted_values_KNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.780476</td>\n",
       "      <td>9.158783</td>\n",
       "      <td>8.083245</td>\n",
       "      <td>5.911610</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>5.739167</td>\n",
       "      <td>6.029593</td>\n",
       "      <td>5.678359</td>\n",
       "      <td>6.269147</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>5.064786</td>\n",
       "      <td>5.561528</td>\n",
       "      <td>4.966199</td>\n",
       "      <td>6.273282</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.174000</td>\n",
       "      <td>6.985331</td>\n",
       "      <td>6.514410</td>\n",
       "      <td>6.105001</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.045714</td>\n",
       "      <td>6.096070</td>\n",
       "      <td>5.765330</td>\n",
       "      <td>6.329648</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  \\\n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...   \n",
       "\n",
       "   AST_predicted_values_Random Forest  AST_predicted_values_XGBoost  \\\n",
       "0                            7.780476                      9.158783   \n",
       "1                            5.739167                      6.029593   \n",
       "2                            5.064786                      5.561528   \n",
       "3                            7.174000                      6.985331   \n",
       "4                            6.045714                      6.096070   \n",
       "\n",
       "   AST_predicted_values_CatBoost  AST_predicted_values_SVR  \\\n",
       "0                       8.083245                  5.911610   \n",
       "1                       5.678359                  6.269147   \n",
       "2                       4.966199                  6.273282   \n",
       "3                       6.514410                  6.105001   \n",
       "4                       5.765330                  6.329648   \n",
       "\n",
       "   AST_predicted_values_KNN  \n",
       "0                       7.2  \n",
       "1                       5.6  \n",
       "2                       7.2  \n",
       "3                       7.2  \n",
       "4                       5.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
