{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fb2e59-ecfe-41ab-a03c-7ac9bdacbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clang.cindex\n",
    "import tempfile\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862cef06-39e8-41df-a709-8628d4794246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  \n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('Data_Ast.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b75cc98-7646-4eb7-91a7-f4a9131b0157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15021da4-0be1-440a-add8-988054aba1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded. Shape: torch.Size([1000, 768])\n",
      "Fold 1\n",
      "Epoch 1/100, Loss: 2.5622\n",
      "Epoch 2/100, Loss: 1.4541\n",
      "Epoch 3/100, Loss: 1.3922\n",
      "Epoch 4/100, Loss: 1.3034\n",
      "Epoch 5/100, Loss: 1.2679\n",
      "Epoch 6/100, Loss: 1.2522\n",
      "Epoch 7/100, Loss: 1.2370\n",
      "Epoch 8/100, Loss: 1.4184\n",
      "Epoch 9/100, Loss: 1.2566\n",
      "Epoch 10/100, Loss: 1.1181\n",
      "Epoch 11/100, Loss: 1.1202\n",
      "Epoch 12/100, Loss: 1.1388\n",
      "Epoch 13/100, Loss: 1.3160\n",
      "Epoch 14/100, Loss: 1.4063\n",
      "Epoch 15/100, Loss: 1.0895\n",
      "Epoch 16/100, Loss: 1.1566\n",
      "Epoch 17/100, Loss: 1.0332\n",
      "Epoch 18/100, Loss: 1.0534\n",
      "Epoch 19/100, Loss: 1.0054\n",
      "Epoch 20/100, Loss: 1.1219\n",
      "Epoch 21/100, Loss: 1.0215\n",
      "Epoch 22/100, Loss: 1.0521\n",
      "Epoch 23/100, Loss: 1.0159\n",
      "Epoch 24/100, Loss: 1.0387\n",
      "Epoch 25/100, Loss: 1.0246\n",
      "Epoch 26/100, Loss: 0.9851\n",
      "Epoch 27/100, Loss: 0.9898\n",
      "Epoch 28/100, Loss: 1.0412\n",
      "Epoch 29/100, Loss: 0.9257\n",
      "Epoch 30/100, Loss: 0.9510\n",
      "Epoch 31/100, Loss: 0.9640\n",
      "Epoch 32/100, Loss: 1.0106\n",
      "Epoch 33/100, Loss: 0.9373\n",
      "Epoch 34/100, Loss: 1.0136\n",
      "Epoch 35/100, Loss: 0.9122\n",
      "Epoch 36/100, Loss: 0.9896\n",
      "Epoch 37/100, Loss: 1.0408\n",
      "Epoch 38/100, Loss: 1.1042\n",
      "Epoch 39/100, Loss: 0.9648\n",
      "Epoch 40/100, Loss: 1.0453\n",
      "Epoch 41/100, Loss: 0.8979\n",
      "Epoch 42/100, Loss: 0.9494\n",
      "Epoch 43/100, Loss: 0.9166\n",
      "Epoch 44/100, Loss: 0.9147\n",
      "Epoch 45/100, Loss: 0.9089\n",
      "Epoch 46/100, Loss: 0.9507\n",
      "Epoch 47/100, Loss: 0.8887\n",
      "Epoch 48/100, Loss: 0.9147\n",
      "Epoch 49/100, Loss: 0.9053\n",
      "Epoch 50/100, Loss: 0.9503\n",
      "Epoch 51/100, Loss: 0.8631\n",
      "Epoch 52/100, Loss: 0.8829\n",
      "Epoch 53/100, Loss: 0.9707\n",
      "Epoch 54/100, Loss: 0.8740\n",
      "Epoch 55/100, Loss: 0.9277\n",
      "Epoch 56/100, Loss: 0.8844\n",
      "Epoch 57/100, Loss: 0.9356\n",
      "Epoch 58/100, Loss: 0.9706\n",
      "Epoch 59/100, Loss: 0.8976\n",
      "Epoch 60/100, Loss: 0.9634\n",
      "Epoch 61/100, Loss: 0.8645\n",
      "Epoch 62/100, Loss: 0.8871\n",
      "Epoch 63/100, Loss: 1.0138\n",
      "Epoch 64/100, Loss: 0.9268\n",
      "Epoch 65/100, Loss: 0.8281\n",
      "Epoch 66/100, Loss: 0.8145\n",
      "Epoch 67/100, Loss: 0.9322\n",
      "Epoch 68/100, Loss: 0.8141\n",
      "Epoch 69/100, Loss: 0.8025\n",
      "Epoch 70/100, Loss: 0.8662\n",
      "Epoch 71/100, Loss: 0.8761\n",
      "Epoch 72/100, Loss: 0.8335\n",
      "Epoch 73/100, Loss: 0.8728\n",
      "Epoch 74/100, Loss: 0.8707\n",
      "Epoch 75/100, Loss: 0.8724\n",
      "Epoch 76/100, Loss: 0.8291\n",
      "Epoch 77/100, Loss: 0.9019\n",
      "Epoch 78/100, Loss: 0.7734\n",
      "Epoch 79/100, Loss: 0.7923\n",
      "Epoch 80/100, Loss: 0.7824\n",
      "Epoch 81/100, Loss: 0.8202\n",
      "Epoch 82/100, Loss: 0.8124\n",
      "Epoch 83/100, Loss: 0.7654\n",
      "Epoch 84/100, Loss: 0.7663\n",
      "Epoch 85/100, Loss: 0.7726\n",
      "Epoch 86/100, Loss: 0.7442\n",
      "Epoch 87/100, Loss: 0.7390\n",
      "Epoch 88/100, Loss: 0.7628\n",
      "Epoch 89/100, Loss: 0.8710\n",
      "Epoch 90/100, Loss: 0.8191\n",
      "Epoch 91/100, Loss: 0.8046\n",
      "Epoch 92/100, Loss: 0.8830\n",
      "Epoch 93/100, Loss: 0.8304\n",
      "Epoch 94/100, Loss: 0.8967\n",
      "Epoch 95/100, Loss: 0.7614\n",
      "Epoch 96/100, Loss: 0.8285\n",
      "Epoch 97/100, Loss: 0.7830\n",
      "Epoch 98/100, Loss: 0.7375\n",
      "Epoch 99/100, Loss: 0.7298\n",
      "Epoch 100/100, Loss: 0.7583\n",
      "Fold 1 R²: 0.4033, RMSE: 1.8045, MAPE: 70994567888896.0000\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.3556\n",
      "Epoch 2/100, Loss: 1.6236\n",
      "Epoch 3/100, Loss: 1.4929\n",
      "Epoch 4/100, Loss: 1.4113\n",
      "Epoch 5/100, Loss: 1.3385\n",
      "Epoch 6/100, Loss: 1.2602\n",
      "Epoch 7/100, Loss: 1.1883\n",
      "Epoch 8/100, Loss: 1.1499\n",
      "Epoch 9/100, Loss: 1.3984\n",
      "Epoch 10/100, Loss: 1.2109\n",
      "Epoch 11/100, Loss: 1.2252\n",
      "Epoch 12/100, Loss: 1.1356\n",
      "Epoch 13/100, Loss: 1.1191\n",
      "Epoch 14/100, Loss: 1.0898\n",
      "Epoch 15/100, Loss: 1.0563\n",
      "Epoch 16/100, Loss: 1.0618\n",
      "Epoch 17/100, Loss: 1.1233\n",
      "Epoch 18/100, Loss: 1.0999\n",
      "Epoch 19/100, Loss: 1.2269\n",
      "Epoch 20/100, Loss: 0.9940\n",
      "Epoch 21/100, Loss: 1.0210\n",
      "Epoch 22/100, Loss: 0.9823\n",
      "Epoch 23/100, Loss: 1.0888\n",
      "Epoch 24/100, Loss: 0.9877\n",
      "Epoch 25/100, Loss: 1.0728\n",
      "Epoch 26/100, Loss: 1.1453\n",
      "Epoch 27/100, Loss: 0.9986\n",
      "Epoch 28/100, Loss: 0.9970\n",
      "Epoch 29/100, Loss: 0.9254\n",
      "Epoch 30/100, Loss: 1.0287\n",
      "Epoch 31/100, Loss: 1.0723\n",
      "Epoch 32/100, Loss: 0.9365\n",
      "Epoch 33/100, Loss: 0.8911\n",
      "Epoch 34/100, Loss: 1.0301\n",
      "Epoch 35/100, Loss: 0.9900\n",
      "Epoch 36/100, Loss: 0.9038\n",
      "Epoch 37/100, Loss: 0.8754\n",
      "Epoch 38/100, Loss: 0.9410\n",
      "Epoch 39/100, Loss: 0.8623\n",
      "Epoch 40/100, Loss: 0.8837\n",
      "Epoch 41/100, Loss: 0.9009\n",
      "Epoch 42/100, Loss: 0.8855\n",
      "Epoch 43/100, Loss: 0.9160\n",
      "Epoch 44/100, Loss: 0.9749\n",
      "Epoch 45/100, Loss: 0.9580\n",
      "Epoch 46/100, Loss: 0.9007\n",
      "Epoch 47/100, Loss: 0.9000\n",
      "Epoch 48/100, Loss: 0.8733\n",
      "Epoch 49/100, Loss: 1.0552\n",
      "Epoch 50/100, Loss: 0.8979\n",
      "Epoch 51/100, Loss: 0.8945\n",
      "Epoch 52/100, Loss: 0.8861\n",
      "Epoch 53/100, Loss: 0.8457\n",
      "Epoch 54/100, Loss: 0.9845\n",
      "Epoch 55/100, Loss: 0.8545\n",
      "Epoch 56/100, Loss: 0.8424\n",
      "Epoch 57/100, Loss: 0.8001\n",
      "Epoch 58/100, Loss: 0.8077\n",
      "Epoch 59/100, Loss: 0.8465\n",
      "Epoch 60/100, Loss: 0.9005\n",
      "Epoch 61/100, Loss: 0.8350\n",
      "Epoch 62/100, Loss: 1.0805\n",
      "Epoch 63/100, Loss: 0.9050\n",
      "Epoch 64/100, Loss: 0.8886\n",
      "Epoch 65/100, Loss: 0.7954\n",
      "Epoch 66/100, Loss: 0.7769\n",
      "Epoch 67/100, Loss: 0.8648\n",
      "Epoch 68/100, Loss: 0.8493\n",
      "Epoch 69/100, Loss: 0.8605\n",
      "Epoch 70/100, Loss: 0.8334\n",
      "Epoch 71/100, Loss: 0.8049\n",
      "Epoch 72/100, Loss: 0.8602\n",
      "Epoch 73/100, Loss: 0.8142\n",
      "Epoch 74/100, Loss: 0.8383\n",
      "Epoch 75/100, Loss: 0.9396\n",
      "Epoch 76/100, Loss: 0.8313\n",
      "Epoch 77/100, Loss: 0.7746\n",
      "Epoch 78/100, Loss: 0.7897\n",
      "Epoch 79/100, Loss: 0.8047\n",
      "Epoch 80/100, Loss: 0.8049\n",
      "Epoch 81/100, Loss: 0.7880\n",
      "Epoch 82/100, Loss: 0.8330\n",
      "Epoch 83/100, Loss: 0.7385\n",
      "Epoch 84/100, Loss: 0.7673\n",
      "Epoch 85/100, Loss: 0.8575\n",
      "Epoch 86/100, Loss: 0.7892\n",
      "Epoch 87/100, Loss: 0.7741\n",
      "Epoch 88/100, Loss: 0.7929\n",
      "Epoch 89/100, Loss: 0.7857\n",
      "Epoch 90/100, Loss: 0.7254\n",
      "Epoch 91/100, Loss: 0.7350\n",
      "Epoch 92/100, Loss: 0.7753\n",
      "Epoch 93/100, Loss: 0.8699\n",
      "Epoch 94/100, Loss: 0.8883\n",
      "Epoch 95/100, Loss: 0.7412\n",
      "Epoch 96/100, Loss: 0.7817\n",
      "Epoch 97/100, Loss: 0.7447\n",
      "Epoch 98/100, Loss: 0.7890\n",
      "Epoch 99/100, Loss: 0.7813\n",
      "Epoch 100/100, Loss: 0.7944\n",
      "Fold 2 R²: 0.1290, RMSE: 2.1497, MAPE: 357580287246336.0000\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.4604\n",
      "Epoch 2/100, Loss: 1.4909\n",
      "Epoch 3/100, Loss: 1.4640\n",
      "Epoch 4/100, Loss: 1.4919\n",
      "Epoch 5/100, Loss: 1.4773\n",
      "Epoch 6/100, Loss: 1.3422\n",
      "Epoch 7/100, Loss: 1.2916\n",
      "Epoch 8/100, Loss: 1.3462\n",
      "Epoch 9/100, Loss: 1.1843\n",
      "Epoch 10/100, Loss: 1.1473\n",
      "Epoch 11/100, Loss: 1.1577\n",
      "Epoch 12/100, Loss: 1.1077\n",
      "Epoch 13/100, Loss: 1.0692\n",
      "Epoch 14/100, Loss: 1.1672\n",
      "Epoch 15/100, Loss: 1.0300\n",
      "Epoch 16/100, Loss: 1.1409\n",
      "Epoch 17/100, Loss: 1.1230\n",
      "Epoch 18/100, Loss: 1.0383\n",
      "Epoch 19/100, Loss: 1.0106\n",
      "Epoch 20/100, Loss: 1.0565\n",
      "Epoch 21/100, Loss: 1.0170\n",
      "Epoch 22/100, Loss: 0.9996\n",
      "Epoch 23/100, Loss: 1.1556\n",
      "Epoch 24/100, Loss: 1.0863\n",
      "Epoch 25/100, Loss: 1.0188\n",
      "Epoch 26/100, Loss: 1.0742\n",
      "Epoch 27/100, Loss: 0.9690\n",
      "Epoch 28/100, Loss: 0.9948\n",
      "Epoch 29/100, Loss: 0.9961\n",
      "Epoch 30/100, Loss: 0.9957\n",
      "Epoch 31/100, Loss: 0.9373\n",
      "Epoch 32/100, Loss: 0.9338\n",
      "Epoch 33/100, Loss: 0.9863\n",
      "Epoch 34/100, Loss: 0.9619\n",
      "Epoch 35/100, Loss: 0.9746\n",
      "Epoch 36/100, Loss: 0.9991\n",
      "Epoch 37/100, Loss: 0.9118\n",
      "Epoch 38/100, Loss: 0.8958\n",
      "Epoch 39/100, Loss: 0.8960\n",
      "Epoch 40/100, Loss: 0.9658\n",
      "Epoch 41/100, Loss: 0.9826\n",
      "Epoch 42/100, Loss: 0.9071\n",
      "Epoch 43/100, Loss: 0.9054\n",
      "Epoch 44/100, Loss: 0.8609\n",
      "Epoch 45/100, Loss: 0.8719\n",
      "Epoch 46/100, Loss: 0.8808\n",
      "Epoch 47/100, Loss: 0.9611\n",
      "Epoch 48/100, Loss: 0.9732\n",
      "Epoch 49/100, Loss: 0.9044\n",
      "Epoch 50/100, Loss: 0.8801\n",
      "Epoch 51/100, Loss: 0.9078\n",
      "Epoch 52/100, Loss: 0.8901\n",
      "Epoch 53/100, Loss: 0.8450\n",
      "Epoch 54/100, Loss: 0.8520\n",
      "Epoch 55/100, Loss: 0.9268\n",
      "Epoch 56/100, Loss: 0.8249\n",
      "Epoch 57/100, Loss: 0.9545\n",
      "Epoch 58/100, Loss: 0.8967\n",
      "Epoch 59/100, Loss: 0.8115\n",
      "Epoch 60/100, Loss: 0.8518\n",
      "Epoch 61/100, Loss: 0.8164\n",
      "Epoch 62/100, Loss: 0.8493\n",
      "Epoch 63/100, Loss: 0.7998\n",
      "Epoch 64/100, Loss: 0.7985\n",
      "Epoch 65/100, Loss: 0.7616\n",
      "Epoch 66/100, Loss: 0.8001\n",
      "Epoch 67/100, Loss: 0.7960\n",
      "Epoch 68/100, Loss: 0.8086\n",
      "Epoch 69/100, Loss: 0.8146\n",
      "Epoch 70/100, Loss: 0.8552\n",
      "Epoch 71/100, Loss: 0.9440\n",
      "Epoch 72/100, Loss: 0.8752\n",
      "Epoch 73/100, Loss: 0.7964\n",
      "Epoch 74/100, Loss: 0.9893\n",
      "Epoch 75/100, Loss: 0.8247\n",
      "Epoch 76/100, Loss: 0.8178\n",
      "Epoch 77/100, Loss: 1.1258\n",
      "Epoch 78/100, Loss: 0.8718\n",
      "Epoch 79/100, Loss: 0.7909\n",
      "Epoch 80/100, Loss: 0.8404\n",
      "Epoch 81/100, Loss: 0.7550\n",
      "Epoch 82/100, Loss: 0.8137\n",
      "Epoch 83/100, Loss: 0.8271\n",
      "Epoch 84/100, Loss: 0.7369\n",
      "Epoch 85/100, Loss: 0.8729\n",
      "Epoch 86/100, Loss: 0.9020\n",
      "Epoch 87/100, Loss: 0.7401\n",
      "Epoch 88/100, Loss: 0.8023\n",
      "Epoch 89/100, Loss: 0.7629\n",
      "Epoch 90/100, Loss: 0.7932\n",
      "Epoch 91/100, Loss: 0.7598\n",
      "Epoch 92/100, Loss: 0.7630\n",
      "Epoch 93/100, Loss: 0.7957\n",
      "Epoch 94/100, Loss: 0.7595\n",
      "Epoch 95/100, Loss: 0.7199\n",
      "Epoch 96/100, Loss: 0.7708\n",
      "Epoch 97/100, Loss: 0.7217\n",
      "Epoch 98/100, Loss: 0.7853\n",
      "Epoch 99/100, Loss: 0.7161\n",
      "Epoch 100/100, Loss: 0.7851\n",
      "Fold 3 R²: 0.3345, RMSE: 2.0204, MAPE: 0.5271\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.5870\n",
      "Epoch 2/100, Loss: 1.5284\n",
      "Epoch 3/100, Loss: 1.4180\n",
      "Epoch 4/100, Loss: 1.3689\n",
      "Epoch 5/100, Loss: 1.3077\n",
      "Epoch 6/100, Loss: 1.5763\n",
      "Epoch 7/100, Loss: 1.3813\n",
      "Epoch 8/100, Loss: 1.3083\n",
      "Epoch 9/100, Loss: 1.2758\n",
      "Epoch 10/100, Loss: 1.2206\n",
      "Epoch 11/100, Loss: 1.1609\n",
      "Epoch 12/100, Loss: 1.2397\n",
      "Epoch 13/100, Loss: 1.1112\n",
      "Epoch 14/100, Loss: 1.1442\n",
      "Epoch 15/100, Loss: 1.1580\n",
      "Epoch 16/100, Loss: 1.0596\n",
      "Epoch 17/100, Loss: 1.1508\n",
      "Epoch 18/100, Loss: 1.0495\n",
      "Epoch 19/100, Loss: 1.1518\n",
      "Epoch 20/100, Loss: 1.0703\n",
      "Epoch 21/100, Loss: 1.0182\n",
      "Epoch 22/100, Loss: 1.0036\n",
      "Epoch 23/100, Loss: 1.0773\n",
      "Epoch 24/100, Loss: 1.0996\n",
      "Epoch 25/100, Loss: 1.0743\n",
      "Epoch 26/100, Loss: 1.0623\n",
      "Epoch 27/100, Loss: 0.9723\n",
      "Epoch 28/100, Loss: 1.0518\n",
      "Epoch 29/100, Loss: 0.9894\n",
      "Epoch 30/100, Loss: 0.9820\n",
      "Epoch 31/100, Loss: 1.0234\n",
      "Epoch 32/100, Loss: 0.9474\n",
      "Epoch 33/100, Loss: 0.9494\n",
      "Epoch 34/100, Loss: 0.9444\n",
      "Epoch 35/100, Loss: 0.9460\n",
      "Epoch 36/100, Loss: 0.9422\n",
      "Epoch 37/100, Loss: 1.0248\n",
      "Epoch 38/100, Loss: 1.0179\n",
      "Epoch 39/100, Loss: 0.9547\n",
      "Epoch 40/100, Loss: 0.9745\n",
      "Epoch 41/100, Loss: 1.0100\n",
      "Epoch 42/100, Loss: 0.9305\n",
      "Epoch 43/100, Loss: 0.9167\n",
      "Epoch 44/100, Loss: 0.9311\n",
      "Epoch 45/100, Loss: 0.9277\n",
      "Epoch 46/100, Loss: 0.9169\n",
      "Epoch 47/100, Loss: 1.0084\n",
      "Epoch 48/100, Loss: 0.9963\n",
      "Epoch 49/100, Loss: 1.1207\n",
      "Epoch 50/100, Loss: 1.0884\n",
      "Epoch 51/100, Loss: 0.9773\n",
      "Epoch 52/100, Loss: 1.0598\n",
      "Epoch 53/100, Loss: 0.8608\n",
      "Epoch 54/100, Loss: 0.9314\n",
      "Epoch 55/100, Loss: 0.9126\n",
      "Epoch 56/100, Loss: 0.9081\n",
      "Epoch 57/100, Loss: 0.8573\n",
      "Epoch 58/100, Loss: 0.8215\n",
      "Epoch 59/100, Loss: 0.8186\n",
      "Epoch 60/100, Loss: 0.9757\n",
      "Epoch 61/100, Loss: 0.9036\n",
      "Epoch 62/100, Loss: 0.8410\n",
      "Epoch 63/100, Loss: 0.8144\n",
      "Epoch 64/100, Loss: 0.8871\n",
      "Epoch 65/100, Loss: 0.8710\n",
      "Epoch 66/100, Loss: 0.7788\n",
      "Epoch 67/100, Loss: 0.7951\n",
      "Epoch 68/100, Loss: 0.8930\n",
      "Epoch 69/100, Loss: 0.9340\n",
      "Epoch 70/100, Loss: 0.8437\n",
      "Epoch 71/100, Loss: 0.9597\n",
      "Epoch 72/100, Loss: 0.8055\n",
      "Epoch 73/100, Loss: 0.8518\n",
      "Epoch 74/100, Loss: 0.7823\n",
      "Epoch 75/100, Loss: 0.7945\n",
      "Epoch 76/100, Loss: 0.7543\n",
      "Epoch 77/100, Loss: 0.8020\n",
      "Epoch 78/100, Loss: 0.8272\n",
      "Epoch 79/100, Loss: 0.7577\n",
      "Epoch 80/100, Loss: 0.8574\n",
      "Epoch 81/100, Loss: 0.7750\n",
      "Epoch 82/100, Loss: 0.8932\n",
      "Epoch 83/100, Loss: 0.7952\n",
      "Epoch 84/100, Loss: 0.7796\n",
      "Epoch 85/100, Loss: 0.7655\n",
      "Epoch 86/100, Loss: 0.7616\n",
      "Epoch 87/100, Loss: 0.8677\n",
      "Epoch 88/100, Loss: 0.9324\n",
      "Epoch 89/100, Loss: 0.7346\n",
      "Epoch 90/100, Loss: 0.7394\n",
      "Epoch 91/100, Loss: 0.7855\n",
      "Epoch 92/100, Loss: 0.8559\n",
      "Epoch 93/100, Loss: 0.7990\n",
      "Epoch 94/100, Loss: 0.7863\n",
      "Epoch 95/100, Loss: 0.7872\n",
      "Epoch 96/100, Loss: 0.7817\n",
      "Epoch 97/100, Loss: 0.7282\n",
      "Epoch 98/100, Loss: 0.7796\n",
      "Epoch 99/100, Loss: 0.7801\n",
      "Epoch 100/100, Loss: 0.7455\n",
      "Fold 4 R²: 0.0352, RMSE: 2.1912, MAPE: 0.4103\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.5264\n",
      "Epoch 2/100, Loss: 1.6670\n",
      "Epoch 3/100, Loss: 1.4926\n",
      "Epoch 4/100, Loss: 1.3897\n",
      "Epoch 5/100, Loss: 1.3355\n",
      "Epoch 6/100, Loss: 1.3937\n",
      "Epoch 7/100, Loss: 1.4685\n",
      "Epoch 8/100, Loss: 1.2795\n",
      "Epoch 9/100, Loss: 1.2450\n",
      "Epoch 10/100, Loss: 1.3210\n",
      "Epoch 11/100, Loss: 1.3209\n",
      "Epoch 12/100, Loss: 1.2725\n",
      "Epoch 13/100, Loss: 1.1502\n",
      "Epoch 14/100, Loss: 1.1204\n",
      "Epoch 15/100, Loss: 1.1854\n",
      "Epoch 16/100, Loss: 1.1150\n",
      "Epoch 17/100, Loss: 1.1292\n",
      "Epoch 18/100, Loss: 1.0801\n",
      "Epoch 19/100, Loss: 1.0974\n",
      "Epoch 20/100, Loss: 1.2808\n",
      "Epoch 21/100, Loss: 1.1772\n",
      "Epoch 22/100, Loss: 1.1702\n",
      "Epoch 23/100, Loss: 1.0351\n",
      "Epoch 24/100, Loss: 0.9950\n",
      "Epoch 25/100, Loss: 1.0819\n",
      "Epoch 26/100, Loss: 1.0037\n",
      "Epoch 27/100, Loss: 0.9685\n",
      "Epoch 28/100, Loss: 1.0601\n",
      "Epoch 29/100, Loss: 1.0040\n",
      "Epoch 30/100, Loss: 0.9939\n",
      "Epoch 31/100, Loss: 1.0595\n",
      "Epoch 32/100, Loss: 0.9479\n",
      "Epoch 33/100, Loss: 1.0737\n",
      "Epoch 34/100, Loss: 0.9772\n",
      "Epoch 35/100, Loss: 1.0875\n",
      "Epoch 36/100, Loss: 0.9832\n",
      "Epoch 37/100, Loss: 0.9448\n",
      "Epoch 38/100, Loss: 0.9686\n",
      "Epoch 39/100, Loss: 0.9192\n",
      "Epoch 40/100, Loss: 0.9536\n",
      "Epoch 41/100, Loss: 0.9032\n",
      "Epoch 42/100, Loss: 0.9815\n",
      "Epoch 43/100, Loss: 1.0066\n",
      "Epoch 44/100, Loss: 0.8992\n",
      "Epoch 45/100, Loss: 0.8820\n",
      "Epoch 46/100, Loss: 0.8802\n",
      "Epoch 47/100, Loss: 0.9073\n",
      "Epoch 48/100, Loss: 0.9742\n",
      "Epoch 49/100, Loss: 0.9659\n",
      "Epoch 50/100, Loss: 0.9337\n",
      "Epoch 51/100, Loss: 0.9474\n",
      "Epoch 52/100, Loss: 0.9238\n",
      "Epoch 53/100, Loss: 0.9472\n",
      "Epoch 54/100, Loss: 0.9273\n",
      "Epoch 55/100, Loss: 0.9211\n",
      "Epoch 56/100, Loss: 0.8514\n",
      "Epoch 57/100, Loss: 0.9063\n",
      "Epoch 58/100, Loss: 0.8779\n",
      "Epoch 59/100, Loss: 0.9085\n",
      "Epoch 60/100, Loss: 0.8263\n",
      "Epoch 61/100, Loss: 0.8768\n",
      "Epoch 62/100, Loss: 0.8620\n",
      "Epoch 63/100, Loss: 0.8728\n",
      "Epoch 64/100, Loss: 0.8199\n",
      "Epoch 65/100, Loss: 0.9024\n",
      "Epoch 66/100, Loss: 0.8330\n",
      "Epoch 67/100, Loss: 0.9058\n",
      "Epoch 68/100, Loss: 0.9630\n",
      "Epoch 69/100, Loss: 0.8719\n",
      "Epoch 70/100, Loss: 0.8461\n",
      "Epoch 71/100, Loss: 0.8404\n",
      "Epoch 72/100, Loss: 0.8179\n",
      "Epoch 73/100, Loss: 0.8385\n",
      "Epoch 74/100, Loss: 0.8081\n",
      "Epoch 75/100, Loss: 0.8106\n",
      "Epoch 76/100, Loss: 0.9062\n",
      "Epoch 77/100, Loss: 0.8310\n",
      "Epoch 78/100, Loss: 0.8962\n",
      "Epoch 79/100, Loss: 0.9681\n",
      "Epoch 80/100, Loss: 1.0679\n",
      "Epoch 81/100, Loss: 0.9429\n",
      "Epoch 82/100, Loss: 0.7961\n",
      "Epoch 83/100, Loss: 0.7745\n",
      "Epoch 84/100, Loss: 0.7659\n",
      "Epoch 85/100, Loss: 0.8067\n",
      "Epoch 86/100, Loss: 0.8500\n",
      "Epoch 87/100, Loss: 0.8080\n",
      "Epoch 88/100, Loss: 0.7692\n",
      "Epoch 89/100, Loss: 0.7728\n",
      "Epoch 90/100, Loss: 0.8415\n",
      "Epoch 91/100, Loss: 0.9297\n",
      "Epoch 92/100, Loss: 0.8388\n",
      "Epoch 93/100, Loss: 0.7848\n",
      "Epoch 94/100, Loss: 0.7503\n",
      "Epoch 95/100, Loss: 0.8211\n",
      "Epoch 96/100, Loss: 0.7665\n",
      "Epoch 97/100, Loss: 0.7641\n",
      "Epoch 98/100, Loss: 0.7897\n",
      "Epoch 99/100, Loss: 0.8344\n",
      "Epoch 100/100, Loss: 0.9113\n",
      "Fold 5 R²: 0.1067, RMSE: 1.8699, MAPE: 0.2999\n",
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.1662\n",
      "Epoch 2/100, Loss: 1.4883\n",
      "Epoch 3/100, Loss: 1.4948\n",
      "Epoch 4/100, Loss: 1.4530\n",
      "Epoch 5/100, Loss: 1.3699\n",
      "Epoch 6/100, Loss: 1.2504\n",
      "Epoch 7/100, Loss: 1.2735\n",
      "Epoch 8/100, Loss: 1.2793\n",
      "Epoch 9/100, Loss: 1.1521\n",
      "Epoch 10/100, Loss: 1.1881\n",
      "Epoch 11/100, Loss: 1.1233\n",
      "Epoch 12/100, Loss: 1.1863\n",
      "Epoch 13/100, Loss: 1.0664\n",
      "Epoch 14/100, Loss: 1.1166\n",
      "Epoch 15/100, Loss: 1.1275\n",
      "Epoch 16/100, Loss: 1.1176\n",
      "Epoch 17/100, Loss: 1.1488\n",
      "Epoch 18/100, Loss: 1.1338\n",
      "Epoch 19/100, Loss: 1.0546\n",
      "Epoch 20/100, Loss: 1.0300\n",
      "Epoch 21/100, Loss: 1.0266\n",
      "Epoch 22/100, Loss: 1.0568\n",
      "Epoch 23/100, Loss: 1.0761\n",
      "Epoch 24/100, Loss: 1.0284\n",
      "Epoch 25/100, Loss: 1.0084\n",
      "Epoch 26/100, Loss: 0.9563\n",
      "Epoch 27/100, Loss: 1.0253\n",
      "Epoch 28/100, Loss: 0.9922\n",
      "Epoch 29/100, Loss: 0.9736\n",
      "Epoch 30/100, Loss: 1.0181\n",
      "Epoch 31/100, Loss: 0.9470\n",
      "Epoch 32/100, Loss: 0.9047\n",
      "Epoch 33/100, Loss: 0.9294\n",
      "Epoch 34/100, Loss: 0.9814\n",
      "Epoch 35/100, Loss: 0.9899\n",
      "Epoch 36/100, Loss: 0.9531\n",
      "Epoch 37/100, Loss: 0.9153\n",
      "Epoch 38/100, Loss: 0.8983\n",
      "Epoch 39/100, Loss: 0.9081\n",
      "Epoch 40/100, Loss: 0.8830\n",
      "Epoch 41/100, Loss: 0.9146\n",
      "Epoch 42/100, Loss: 0.8653\n",
      "Epoch 43/100, Loss: 0.8931\n",
      "Epoch 44/100, Loss: 0.8766\n",
      "Epoch 45/100, Loss: 0.8874\n",
      "Epoch 46/100, Loss: 0.8779\n",
      "Epoch 47/100, Loss: 0.8756\n",
      "Epoch 48/100, Loss: 0.9299\n",
      "Epoch 49/100, Loss: 1.0169\n",
      "Epoch 50/100, Loss: 1.0256\n",
      "Epoch 51/100, Loss: 0.9757\n",
      "Epoch 52/100, Loss: 0.9185\n",
      "Epoch 53/100, Loss: 0.8508\n",
      "Epoch 54/100, Loss: 1.1218\n",
      "Epoch 55/100, Loss: 0.9759\n",
      "Epoch 56/100, Loss: 0.8566\n",
      "Epoch 57/100, Loss: 0.8533\n",
      "Epoch 58/100, Loss: 0.8706\n",
      "Epoch 59/100, Loss: 0.9598\n",
      "Epoch 60/100, Loss: 0.9913\n",
      "Epoch 61/100, Loss: 1.0351\n",
      "Epoch 62/100, Loss: 0.8721\n",
      "Epoch 63/100, Loss: 0.8916\n",
      "Epoch 64/100, Loss: 0.8663\n",
      "Epoch 65/100, Loss: 0.8662\n",
      "Epoch 66/100, Loss: 0.8529\n",
      "Epoch 67/100, Loss: 0.8242\n",
      "Epoch 68/100, Loss: 0.8340\n",
      "Epoch 69/100, Loss: 0.9735\n",
      "Epoch 70/100, Loss: 0.8563\n",
      "Epoch 71/100, Loss: 0.7875\n",
      "Epoch 72/100, Loss: 0.8624\n",
      "Epoch 73/100, Loss: 0.9172\n",
      "Epoch 74/100, Loss: 0.9301\n",
      "Epoch 75/100, Loss: 0.9769\n",
      "Epoch 76/100, Loss: 0.8361\n",
      "Epoch 77/100, Loss: 0.8642\n",
      "Epoch 78/100, Loss: 0.7647\n",
      "Epoch 79/100, Loss: 0.8136\n",
      "Epoch 80/100, Loss: 0.8954\n",
      "Epoch 81/100, Loss: 0.8334\n",
      "Epoch 82/100, Loss: 0.8464\n",
      "Epoch 83/100, Loss: 0.8101\n",
      "Epoch 84/100, Loss: 0.8646\n",
      "Epoch 85/100, Loss: 0.8477\n",
      "Epoch 86/100, Loss: 0.8588\n",
      "Epoch 87/100, Loss: 0.8177\n",
      "Epoch 88/100, Loss: 0.7908\n",
      "Epoch 89/100, Loss: 0.7709\n",
      "Epoch 90/100, Loss: 0.8353\n",
      "Epoch 91/100, Loss: 0.8076\n",
      "Epoch 92/100, Loss: 0.7232\n",
      "Epoch 93/100, Loss: 0.8089\n",
      "Epoch 94/100, Loss: 0.7823\n",
      "Epoch 95/100, Loss: 0.7809\n",
      "Epoch 96/100, Loss: 0.7303\n",
      "Epoch 97/100, Loss: 0.7721\n",
      "Epoch 98/100, Loss: 0.7417\n",
      "Epoch 99/100, Loss: 0.7876\n",
      "Epoch 100/100, Loss: 0.7425\n",
      "Fold 6 R²: 0.3471, RMSE: 1.7904, MAPE: 0.4076\n",
      "Fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.9215\n",
      "Epoch 2/100, Loss: 1.5635\n",
      "Epoch 3/100, Loss: 1.4338\n",
      "Epoch 4/100, Loss: 1.4204\n",
      "Epoch 5/100, Loss: 1.4368\n",
      "Epoch 6/100, Loss: 1.2298\n",
      "Epoch 7/100, Loss: 1.2755\n",
      "Epoch 8/100, Loss: 1.2207\n",
      "Epoch 9/100, Loss: 1.2381\n",
      "Epoch 10/100, Loss: 1.1429\n",
      "Epoch 11/100, Loss: 1.1825\n",
      "Epoch 12/100, Loss: 1.2523\n",
      "Epoch 13/100, Loss: 1.1184\n",
      "Epoch 14/100, Loss: 1.1909\n",
      "Epoch 15/100, Loss: 1.1539\n",
      "Epoch 16/100, Loss: 1.2757\n",
      "Epoch 17/100, Loss: 1.0629\n",
      "Epoch 18/100, Loss: 1.0668\n",
      "Epoch 19/100, Loss: 1.1567\n",
      "Epoch 20/100, Loss: 1.0371\n",
      "Epoch 21/100, Loss: 1.0381\n",
      "Epoch 22/100, Loss: 0.9888\n",
      "Epoch 23/100, Loss: 1.0116\n",
      "Epoch 24/100, Loss: 1.0048\n",
      "Epoch 25/100, Loss: 1.1174\n",
      "Epoch 26/100, Loss: 1.0290\n",
      "Epoch 27/100, Loss: 0.9880\n",
      "Epoch 28/100, Loss: 0.9356\n",
      "Epoch 29/100, Loss: 0.9246\n",
      "Epoch 30/100, Loss: 0.9991\n",
      "Epoch 31/100, Loss: 0.9452\n",
      "Epoch 32/100, Loss: 0.9584\n",
      "Epoch 33/100, Loss: 0.9641\n",
      "Epoch 34/100, Loss: 0.9665\n",
      "Epoch 35/100, Loss: 1.0006\n",
      "Epoch 36/100, Loss: 1.0031\n",
      "Epoch 37/100, Loss: 0.9945\n",
      "Epoch 38/100, Loss: 0.9229\n",
      "Epoch 39/100, Loss: 0.8600\n",
      "Epoch 40/100, Loss: 0.9092\n",
      "Epoch 41/100, Loss: 1.0103\n",
      "Epoch 42/100, Loss: 0.9252\n",
      "Epoch 43/100, Loss: 0.9189\n",
      "Epoch 44/100, Loss: 0.8409\n",
      "Epoch 45/100, Loss: 0.9214\n",
      "Epoch 46/100, Loss: 0.8760\n",
      "Epoch 47/100, Loss: 0.8911\n",
      "Epoch 48/100, Loss: 0.8438\n",
      "Epoch 49/100, Loss: 0.8299\n",
      "Epoch 50/100, Loss: 0.8663\n",
      "Epoch 51/100, Loss: 0.8499\n",
      "Epoch 52/100, Loss: 0.9228\n",
      "Epoch 53/100, Loss: 0.8354\n",
      "Epoch 54/100, Loss: 0.9061\n",
      "Epoch 55/100, Loss: 0.9246\n",
      "Epoch 56/100, Loss: 1.0150\n",
      "Epoch 57/100, Loss: 0.8753\n",
      "Epoch 58/100, Loss: 0.8574\n",
      "Epoch 59/100, Loss: 0.9541\n",
      "Epoch 60/100, Loss: 0.8862\n",
      "Epoch 61/100, Loss: 0.8064\n",
      "Epoch 62/100, Loss: 0.9379\n",
      "Epoch 63/100, Loss: 0.8788\n",
      "Epoch 64/100, Loss: 0.8619\n",
      "Epoch 65/100, Loss: 0.8821\n",
      "Epoch 66/100, Loss: 0.8363\n",
      "Epoch 67/100, Loss: 0.8434\n",
      "Epoch 68/100, Loss: 0.8238\n",
      "Epoch 69/100, Loss: 0.8321\n",
      "Epoch 70/100, Loss: 0.8295\n",
      "Epoch 71/100, Loss: 0.8258\n",
      "Epoch 72/100, Loss: 0.8129\n",
      "Epoch 73/100, Loss: 0.8033\n",
      "Epoch 74/100, Loss: 0.7910\n",
      "Epoch 75/100, Loss: 0.7945\n",
      "Epoch 76/100, Loss: 0.7818\n",
      "Epoch 77/100, Loss: 0.8991\n",
      "Epoch 78/100, Loss: 0.8303\n",
      "Epoch 79/100, Loss: 0.8054\n",
      "Epoch 80/100, Loss: 0.8921\n",
      "Epoch 81/100, Loss: 0.7951\n",
      "Epoch 82/100, Loss: 0.8534\n",
      "Epoch 83/100, Loss: 0.8206\n",
      "Epoch 84/100, Loss: 0.7813\n",
      "Epoch 85/100, Loss: 0.8155\n",
      "Epoch 86/100, Loss: 0.7569\n",
      "Epoch 87/100, Loss: 0.7633\n",
      "Epoch 88/100, Loss: 0.8280\n",
      "Epoch 89/100, Loss: 0.8340\n",
      "Epoch 90/100, Loss: 0.9249\n",
      "Epoch 91/100, Loss: 0.7382\n",
      "Epoch 92/100, Loss: 0.7360\n",
      "Epoch 93/100, Loss: 0.7649\n",
      "Epoch 94/100, Loss: 0.7756\n",
      "Epoch 95/100, Loss: 0.7133\n",
      "Epoch 96/100, Loss: 0.7557\n",
      "Epoch 97/100, Loss: 0.7441\n",
      "Epoch 98/100, Loss: 0.8111\n",
      "Epoch 99/100, Loss: 0.7571\n",
      "Epoch 100/100, Loss: 0.7961\n",
      "Fold 7 R²: 0.1594, RMSE: 2.0434, MAPE: 261510576209920.0000\n",
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.9548\n",
      "Epoch 2/100, Loss: 1.6033\n",
      "Epoch 3/100, Loss: 1.4864\n",
      "Epoch 4/100, Loss: 1.3934\n",
      "Epoch 5/100, Loss: 1.4508\n",
      "Epoch 6/100, Loss: 1.3498\n",
      "Epoch 7/100, Loss: 1.2632\n",
      "Epoch 8/100, Loss: 1.2382\n",
      "Epoch 9/100, Loss: 1.4664\n",
      "Epoch 10/100, Loss: 1.2727\n",
      "Epoch 11/100, Loss: 1.1656\n",
      "Epoch 12/100, Loss: 1.1440\n",
      "Epoch 13/100, Loss: 1.1222\n",
      "Epoch 14/100, Loss: 1.0920\n",
      "Epoch 15/100, Loss: 1.1076\n",
      "Epoch 16/100, Loss: 1.0368\n",
      "Epoch 17/100, Loss: 1.3131\n",
      "Epoch 18/100, Loss: 1.2230\n",
      "Epoch 19/100, Loss: 1.0881\n",
      "Epoch 20/100, Loss: 1.1330\n",
      "Epoch 21/100, Loss: 1.0515\n",
      "Epoch 22/100, Loss: 1.0320\n",
      "Epoch 23/100, Loss: 0.9890\n",
      "Epoch 24/100, Loss: 0.9611\n",
      "Epoch 25/100, Loss: 0.9979\n",
      "Epoch 26/100, Loss: 1.0119\n",
      "Epoch 27/100, Loss: 1.0509\n",
      "Epoch 28/100, Loss: 1.1138\n",
      "Epoch 29/100, Loss: 1.1421\n",
      "Epoch 30/100, Loss: 1.0934\n",
      "Epoch 31/100, Loss: 1.0097\n",
      "Epoch 32/100, Loss: 1.1193\n",
      "Epoch 33/100, Loss: 0.9206\n",
      "Epoch 34/100, Loss: 1.1039\n",
      "Epoch 35/100, Loss: 0.9696\n",
      "Epoch 36/100, Loss: 0.9270\n",
      "Epoch 37/100, Loss: 0.9610\n",
      "Epoch 38/100, Loss: 0.9664\n",
      "Epoch 39/100, Loss: 0.9237\n",
      "Epoch 40/100, Loss: 1.0313\n",
      "Epoch 41/100, Loss: 1.0748\n",
      "Epoch 42/100, Loss: 1.0403\n",
      "Epoch 43/100, Loss: 0.8999\n",
      "Epoch 44/100, Loss: 1.0046\n",
      "Epoch 45/100, Loss: 0.8971\n",
      "Epoch 46/100, Loss: 0.9758\n",
      "Epoch 47/100, Loss: 1.0110\n",
      "Epoch 48/100, Loss: 0.9804\n",
      "Epoch 49/100, Loss: 0.9348\n",
      "Epoch 50/100, Loss: 1.0320\n",
      "Epoch 51/100, Loss: 0.8980\n",
      "Epoch 52/100, Loss: 0.9332\n",
      "Epoch 53/100, Loss: 0.8581\n",
      "Epoch 54/100, Loss: 0.8866\n",
      "Epoch 55/100, Loss: 1.0294\n",
      "Epoch 56/100, Loss: 1.0799\n",
      "Epoch 57/100, Loss: 0.9251\n",
      "Epoch 58/100, Loss: 0.8966\n",
      "Epoch 59/100, Loss: 0.9077\n",
      "Epoch 60/100, Loss: 0.8837\n",
      "Epoch 61/100, Loss: 0.9211\n",
      "Epoch 62/100, Loss: 0.8921\n",
      "Epoch 63/100, Loss: 0.8224\n",
      "Epoch 64/100, Loss: 0.8827\n",
      "Epoch 65/100, Loss: 0.8631\n",
      "Epoch 66/100, Loss: 0.8724\n",
      "Epoch 67/100, Loss: 0.8949\n",
      "Epoch 69/100, Loss: 0.8419\n",
      "Epoch 70/100, Loss: 0.8356\n",
      "Epoch 71/100, Loss: 0.9008\n",
      "Epoch 72/100, Loss: 0.8834\n",
      "Epoch 73/100, Loss: 0.8685\n",
      "Epoch 74/100, Loss: 0.8067\n",
      "Epoch 75/100, Loss: 0.8341\n",
      "Epoch 76/100, Loss: 0.8322\n",
      "Epoch 77/100, Loss: 0.7839\n",
      "Epoch 78/100, Loss: 0.8508\n",
      "Epoch 79/100, Loss: 0.8149\n",
      "Epoch 80/100, Loss: 0.8139\n",
      "Epoch 81/100, Loss: 0.8858\n",
      "Epoch 82/100, Loss: 0.8548\n",
      "Epoch 83/100, Loss: 0.8364\n",
      "Epoch 84/100, Loss: 0.7606\n",
      "Epoch 85/100, Loss: 0.7659\n",
      "Epoch 86/100, Loss: 0.8027\n",
      "Epoch 87/100, Loss: 0.7952\n",
      "Epoch 88/100, Loss: 0.9142\n",
      "Epoch 89/100, Loss: 0.8077\n",
      "Epoch 90/100, Loss: 0.7748\n",
      "Epoch 91/100, Loss: 0.7504\n",
      "Epoch 92/100, Loss: 0.8114\n",
      "Epoch 93/100, Loss: 0.8136\n",
      "Epoch 94/100, Loss: 0.7611\n",
      "Epoch 95/100, Loss: 0.7887\n",
      "Epoch 96/100, Loss: 0.7601\n",
      "Epoch 97/100, Loss: 0.7941\n",
      "Epoch 98/100, Loss: 0.7742\n",
      "Epoch 99/100, Loss: 0.7502\n",
      "Epoch 100/100, Loss: 0.7845\n",
      "Fold 8 R²: 0.1552, RMSE: 2.0493, MAPE: 0.3987\n",
      "Fold 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.7093\n",
      "Epoch 2/100, Loss: 1.5765\n",
      "Epoch 3/100, Loss: 1.4716\n",
      "Epoch 4/100, Loss: 1.3817\n",
      "Epoch 5/100, Loss: 1.4244\n",
      "Epoch 6/100, Loss: 1.3885\n",
      "Epoch 7/100, Loss: 1.4241\n",
      "Epoch 8/100, Loss: 1.4334\n",
      "Epoch 9/100, Loss: 1.2019\n",
      "Epoch 10/100, Loss: 1.2734\n",
      "Epoch 11/100, Loss: 1.1489\n",
      "Epoch 12/100, Loss: 1.1467\n",
      "Epoch 13/100, Loss: 1.0959\n",
      "Epoch 14/100, Loss: 1.2027\n",
      "Epoch 15/100, Loss: 1.2165\n",
      "Epoch 16/100, Loss: 1.0825\n",
      "Epoch 17/100, Loss: 1.1068\n",
      "Epoch 18/100, Loss: 1.1744\n",
      "Epoch 19/100, Loss: 1.0165\n",
      "Epoch 20/100, Loss: 1.1741\n",
      "Epoch 21/100, Loss: 1.0874\n",
      "Epoch 22/100, Loss: 0.9608\n",
      "Epoch 23/100, Loss: 1.1058\n",
      "Epoch 24/100, Loss: 1.0032\n",
      "Epoch 25/100, Loss: 0.9859\n",
      "Epoch 26/100, Loss: 0.9698\n",
      "Epoch 27/100, Loss: 0.9620\n",
      "Epoch 28/100, Loss: 1.0316\n",
      "Epoch 29/100, Loss: 1.0103\n",
      "Epoch 30/100, Loss: 1.0249\n",
      "Epoch 31/100, Loss: 1.1420\n",
      "Epoch 32/100, Loss: 1.0895\n",
      "Epoch 33/100, Loss: 0.9875\n",
      "Epoch 34/100, Loss: 1.0076\n",
      "Epoch 35/100, Loss: 0.9432\n",
      "Epoch 36/100, Loss: 0.9071\n",
      "Epoch 37/100, Loss: 0.8778\n",
      "Epoch 38/100, Loss: 0.9915\n",
      "Epoch 39/100, Loss: 0.9522\n",
      "Epoch 40/100, Loss: 0.9025\n",
      "Epoch 41/100, Loss: 0.9333\n",
      "Epoch 42/100, Loss: 0.9561\n",
      "Epoch 43/100, Loss: 0.8972\n",
      "Epoch 44/100, Loss: 0.8680\n",
      "Epoch 45/100, Loss: 0.9044\n",
      "Epoch 46/100, Loss: 0.8683\n",
      "Epoch 47/100, Loss: 0.9983\n",
      "Epoch 48/100, Loss: 0.8691\n",
      "Epoch 49/100, Loss: 0.8751\n",
      "Epoch 50/100, Loss: 0.8677\n",
      "Epoch 51/100, Loss: 0.9231\n",
      "Epoch 52/100, Loss: 0.9242\n",
      "Epoch 53/100, Loss: 0.8849\n",
      "Epoch 54/100, Loss: 0.8375\n",
      "Epoch 55/100, Loss: 0.9612\n",
      "Epoch 56/100, Loss: 0.8329\n",
      "Epoch 57/100, Loss: 0.8416\n",
      "Epoch 58/100, Loss: 0.8444\n",
      "Epoch 59/100, Loss: 0.8192\n",
      "Epoch 60/100, Loss: 0.8867\n",
      "Epoch 61/100, Loss: 0.8189\n",
      "Epoch 62/100, Loss: 0.8844\n",
      "Epoch 63/100, Loss: 0.8219\n",
      "Epoch 64/100, Loss: 0.8441\n",
      "Epoch 65/100, Loss: 0.8885\n",
      "Epoch 66/100, Loss: 0.9210\n",
      "Epoch 67/100, Loss: 0.8160\n",
      "Epoch 68/100, Loss: 0.8486\n",
      "Epoch 69/100, Loss: 0.8152\n",
      "Epoch 70/100, Loss: 0.8017\n",
      "Epoch 71/100, Loss: 0.8213\n",
      "Epoch 72/100, Loss: 0.8744\n",
      "Epoch 73/100, Loss: 0.7709\n",
      "Epoch 74/100, Loss: 0.7958\n",
      "Epoch 75/100, Loss: 0.8517\n",
      "Epoch 76/100, Loss: 0.8589\n",
      "Epoch 77/100, Loss: 0.7806\n",
      "Epoch 78/100, Loss: 0.7900\n",
      "Epoch 79/100, Loss: 0.8133\n",
      "Epoch 80/100, Loss: 0.7485\n",
      "Epoch 81/100, Loss: 0.7763\n",
      "Epoch 82/100, Loss: 0.8205\n",
      "Epoch 83/100, Loss: 0.7671\n",
      "Epoch 84/100, Loss: 0.7393\n",
      "Epoch 85/100, Loss: 0.7356\n",
      "Epoch 86/100, Loss: 0.7638\n",
      "Epoch 87/100, Loss: 0.7350\n",
      "Epoch 88/100, Loss: 0.7289\n",
      "Epoch 89/100, Loss: 0.7639\n",
      "Epoch 90/100, Loss: 0.8330\n",
      "Epoch 91/100, Loss: 0.7422\n",
      "Epoch 92/100, Loss: 0.7397\n",
      "Epoch 93/100, Loss: 0.7082\n",
      "Epoch 94/100, Loss: 0.7735\n",
      "Epoch 95/100, Loss: 0.7364\n",
      "Epoch 96/100, Loss: 0.7165\n",
      "Epoch 97/100, Loss: 0.7608\n",
      "Epoch 98/100, Loss: 0.9968\n",
      "Epoch 99/100, Loss: 0.8245\n",
      "Epoch 100/100, Loss: 0.8325\n",
      "Fold 9 R²: 0.0525, RMSE: 2.1699, MAPE: 299320733597696.0000\n",
      "Fold 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.9328\n",
      "Epoch 2/100, Loss: 1.6024\n",
      "Epoch 3/100, Loss: 1.4755\n",
      "Epoch 4/100, Loss: 1.4064\n",
      "Epoch 5/100, Loss: 1.3680\n",
      "Epoch 6/100, Loss: 1.2744\n",
      "Epoch 7/100, Loss: 1.2126\n",
      "Epoch 8/100, Loss: 1.2330\n",
      "Epoch 9/100, Loss: 1.2609\n",
      "Epoch 10/100, Loss: 1.1657\n",
      "Epoch 11/100, Loss: 1.1297\n",
      "Epoch 12/100, Loss: 1.1138\n",
      "Epoch 13/100, Loss: 1.1531\n",
      "Epoch 14/100, Loss: 1.0689\n",
      "Epoch 15/100, Loss: 1.3336\n",
      "Epoch 16/100, Loss: 1.2155\n",
      "Epoch 17/100, Loss: 1.0980\n",
      "Epoch 18/100, Loss: 1.0409\n",
      "Epoch 19/100, Loss: 0.9840\n",
      "Epoch 20/100, Loss: 1.0226\n",
      "Epoch 21/100, Loss: 1.0237\n",
      "Epoch 22/100, Loss: 1.1067\n",
      "Epoch 23/100, Loss: 1.0040\n",
      "Epoch 24/100, Loss: 0.9448\n",
      "Epoch 25/100, Loss: 1.0110\n",
      "Epoch 26/100, Loss: 1.0243\n",
      "Epoch 27/100, Loss: 0.9990\n",
      "Epoch 28/100, Loss: 0.9476\n",
      "Epoch 29/100, Loss: 0.9388\n",
      "Epoch 30/100, Loss: 0.9584\n",
      "Epoch 31/100, Loss: 0.9811\n",
      "Epoch 32/100, Loss: 0.9154\n",
      "Epoch 33/100, Loss: 1.0247\n",
      "Epoch 34/100, Loss: 0.9590\n",
      "Epoch 35/100, Loss: 0.9100\n",
      "Epoch 36/100, Loss: 0.9392\n",
      "Epoch 37/100, Loss: 0.9602\n",
      "Epoch 38/100, Loss: 0.9219\n",
      "Epoch 39/100, Loss: 0.8667\n",
      "Epoch 40/100, Loss: 0.9315\n",
      "Epoch 41/100, Loss: 0.9454\n",
      "Epoch 42/100, Loss: 0.8948\n",
      "Epoch 43/100, Loss: 0.9855\n",
      "Epoch 44/100, Loss: 1.0140\n",
      "Epoch 45/100, Loss: 0.8646\n",
      "Epoch 46/100, Loss: 0.8884\n",
      "Epoch 47/100, Loss: 0.9228\n",
      "Epoch 48/100, Loss: 1.1025\n",
      "Epoch 49/100, Loss: 0.8570\n",
      "Epoch 50/100, Loss: 0.8632\n",
      "Epoch 51/100, Loss: 0.9160\n",
      "Epoch 52/100, Loss: 0.8724\n",
      "Epoch 53/100, Loss: 0.8431\n",
      "Epoch 54/100, Loss: 0.8264\n",
      "Epoch 55/100, Loss: 0.8302\n",
      "Epoch 56/100, Loss: 0.8194\n",
      "Epoch 57/100, Loss: 0.8232\n",
      "Epoch 58/100, Loss: 0.7828\n",
      "Epoch 59/100, Loss: 0.7881\n",
      "Epoch 60/100, Loss: 0.8234\n",
      "Epoch 61/100, Loss: 0.7959\n",
      "Epoch 62/100, Loss: 0.8299\n",
      "Epoch 63/100, Loss: 0.8400\n",
      "Epoch 64/100, Loss: 0.8096\n",
      "Epoch 65/100, Loss: 0.7715\n",
      "Epoch 66/100, Loss: 0.7715\n",
      "Epoch 67/100, Loss: 0.7721\n",
      "Epoch 68/100, Loss: 0.7730\n",
      "Epoch 69/100, Loss: 0.7523\n",
      "Epoch 70/100, Loss: 0.7661\n",
      "Epoch 71/100, Loss: 0.7976\n",
      "Epoch 72/100, Loss: 0.8584\n",
      "Epoch 73/100, Loss: 0.7598\n",
      "Epoch 74/100, Loss: 0.8782\n",
      "Epoch 75/100, Loss: 0.8276\n",
      "Epoch 76/100, Loss: 0.7686\n",
      "Epoch 77/100, Loss: 0.7303\n",
      "Epoch 78/100, Loss: 0.8161\n",
      "Epoch 79/100, Loss: 0.7536\n",
      "Epoch 80/100, Loss: 0.7885\n",
      "Epoch 81/100, Loss: 0.7453\n",
      "Epoch 82/100, Loss: 0.8224\n",
      "Epoch 83/100, Loss: 0.8129\n",
      "Epoch 84/100, Loss: 0.7989\n",
      "Epoch 85/100, Loss: 0.7900\n",
      "Epoch 86/100, Loss: 0.7722\n",
      "Epoch 87/100, Loss: 0.7623\n",
      "Epoch 88/100, Loss: 0.8379\n",
      "Epoch 89/100, Loss: 0.7516\n",
      "Epoch 90/100, Loss: 0.7847\n",
      "Epoch 91/100, Loss: 0.8616\n",
      "Epoch 92/100, Loss: 0.7506\n",
      "Epoch 93/100, Loss: 0.7117\n",
      "Epoch 94/100, Loss: 0.7132\n",
      "Epoch 95/100, Loss: 0.7432\n",
      "Epoch 96/100, Loss: 0.8056\n",
      "Epoch 97/100, Loss: 0.8187\n",
      "Epoch 98/100, Loss: 0.7316\n",
      "Epoch 99/100, Loss: 0.8139\n",
      "Epoch 100/100, Loss: 0.7686\n",
      "Fold 10 R²: 0.1284, RMSE: 2.2112, MAPE: 19895953653760.0000\n",
      "Predictions saved to 'data_with_predictions.csv'!\n",
      "Final Metrics - R²: 0.1981, RMSE: 2.0356, MAPE: 100930210500706.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/Users/jagadeeshgurram/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd  # Assuming pandas is used for data storage\n",
    "\n",
    "# Define your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load embeddings and labels\n",
    "loaded_embeddings = torch.load('embeddings_code.pt', map_location=device)\n",
    "code_embeddings = loaded_embeddings['code_embeddings']  # Shape: [1, 1000, 1, 512, 768]\n",
    "code_embeddings = code_embeddings.squeeze(0).squeeze(1)  # Shape: [1000, 512, 768]\n",
    "code_embeddings = code_embeddings[:, 0, :]  # Select CLS token, Shape: [1000, 768]\n",
    "print(f\"Embeddings loaded. Shape: {code_embeddings.shape}\")\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame that contains the 'Total_Marks' column and a placeholder for predictions\n",
    "\n",
    "labels = torch.tensor(data['Total_Marks']).float()  # Assuming 'Total_Marks' column contains the target labels\n",
    "assert len(labels) == code_embeddings.size(0), \"Number of labels must match number of embeddings\"\n",
    "\n",
    "# Normalize labels (min-max normalization)\n",
    "labels_min = labels.min()\n",
    "labels_max = labels.max()\n",
    "labels = (labels - labels_min) / (labels_max - labels_min)  # Normalize to range [0, 1]\n",
    "\n",
    "# Custom Dataset for embeddings and labels\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# BiLSTM Model for regression with an extra hidden layer\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 128)  # Additional hidden layer\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)  # [batch_size, seq_len, 2*hidden_dim]\n",
    "        lstm_out = lstm_out.mean(dim=1)  # Mean pooling over sequence length, shape: [batch_size, 2*hidden_dim]\n",
    "        fc1_out = torch.relu(self.fc1(lstm_out))  # Pass through first hidden layer\n",
    "        output = self.fc2(fc1_out)  # [batch_size, 1]\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = code_embeddings.size(-1)  # 768\n",
    "hidden_dim = 256  # Hidden state size\n",
    "num_layers = 2   # Number of BiLSTM layers\n",
    "batch_size = 32  # Batch size for training\n",
    "num_epochs = 100  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 10-Fold Cross-Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "predicted_values = np.zeros(len(labels))  # Store predictions for all folds\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(code_embeddings)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Prepare data\n",
    "    train_dataset = CodeDataset(code_embeddings[train_idx], labels[train_idx])\n",
    "    test_dataset = CodeDataset(code_embeddings[test_idx], labels[test_idx])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Reset model and optimizer\n",
    "    model = BiLSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for inputs, target in train_loader:\n",
    "            inputs = inputs.to(device).float().unsqueeze(1)  # Add sequence length dimension\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs).squeeze(-1)  # [batch_size]\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs = inputs.to(device).float().unsqueeze(1)\n",
    "            target = target.to(device).float()\n",
    "\n",
    "            output = model(inputs).squeeze(-1)  # [batch_size]\n",
    "            test_preds.extend(output.cpu().numpy())\n",
    "            test_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # De-normalize predictions and targets\n",
    "    test_preds = np.array(test_preds) * (labels_max - labels_min).item() + labels_min.item()\n",
    "    test_targets = np.array(test_targets) * (labels_max - labels_min).item() + labels_min.item()\n",
    "\n",
    "    # Store predictions back into the dataset\n",
    "    data.loc[test_idx, 'Predictions'] = test_preds\n",
    "\n",
    "    # Store predictions in their respective indices for metrics calculation\n",
    "    predicted_values[test_idx] = test_preds\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(test_targets, test_preds)\n",
    "    rmse = mean_squared_error(test_targets, test_preds, squared=False)\n",
    "    mape = mean_absolute_percentage_error(test_targets, test_preds)\n",
    "\n",
    "    print(f\"Fold {fold + 1} R²: {r2:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.4f}\")\n",
    "\n",
    "# Save the predicted values back to 'data'\n",
    "data.to_csv('data_with_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'data_with_predictions.csv'!\")\n",
    "\n",
    "# Final metrics on the entire dataset\n",
    "r2_final = r2_score(labels * (labels_max - labels_min).item() + labels_min.item(), predicted_values)\n",
    "rmse_final = mean_squared_error(labels * (labels_max - labels_min).item() + labels_min.item(), predicted_values, squared=False)\n",
    "mape_final = mean_absolute_percentage_error(labels * (labels_max - labels_min).item() + labels_min.item(), predicted_values)\n",
    "\n",
    "print(f\"Final Metrics - R²: {r2_final:.4f}, RMSE: {rmse_final:.4f}, MAPE: {mape_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "200503a5-9bb8-4021-9d20-f7cbc73dea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.848756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.890309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.751390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>6.147062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "      <td>7.496598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  Predictions  \n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     7.848756  \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     6.890309  \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     6.751390  \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     6.147062  \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...     7.496598  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a2589f2-7d01-4c50-b66c-80026ef6ef1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1865222771684788"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(data['Total_Marks'], data['Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af9bc77-37e8-47a9-a3bf-aa32f545fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Total_Marks']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a8a9cd-99e0-4631-b416-3825bfea2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "file_names = [f'batch_{i}.pt' for i in range(10)]  # batch_0.pt, batch_1.pt, ..., batch_8.pt\n",
    "\n",
    "# Initialize an empty list to store the loaded tensors\n",
    "all_tensors = []\n",
    "\n",
    "# Load and append each tensor to the list\n",
    "for file_name in file_names:\n",
    "    tensor = torch.load(file_name)  # Load the tensor from the file\n",
    "    all_tensors.append(tensor)  # Append it to the list\n",
    "\n",
    "# Concatenate all tensors along the batch dimension (assuming they're of the same shape)\n",
    "ast_embeddings = torch.cat(all_tensors, dim=0)  # dim=0 means concatenating along the batch dimension\n",
    "\n",
    "# Save the combined tensor to a new .pt file\n",
    "torch.save(ast_embeddings, 'combined_batches.pt')\n",
    "\n",
    "print(f\"Combined tensor saved as 'combined_batches.pt' with shape: {combined_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf087b-27c0-4523-99ab-174c9b2ea913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
