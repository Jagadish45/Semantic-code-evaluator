{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f0365a",
   "metadata": {
    "id": "23f0365a"
   },
   "source": [
    "### Importing essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db44763-5edc-4f70-944a-ad841b20d210",
   "metadata": {
    "id": "0db44763-5edc-4f70-944a-ad841b20d210"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clang.cindex\n",
    "import tempfile\n",
    "from sklearn.model_selection import KFold\n",
    "import logging\n",
    "import re\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175afdc6",
   "metadata": {
    "id": "175afdc6"
   },
   "source": [
    "### Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754e0a04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "754e0a04",
    "outputId": "d21229e1-20ae-4917-c47c-622d2e88cded"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Correct_Code</th>\n",
       "      <th>Code_with_Error</th>\n",
       "      <th>Total_Marks</th>\n",
       "      <th>AST_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Print the factors of a number</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\nvoid printFactors(int numb...</td>\n",
       "      <td>#include &lt;stdio.h&gt;\\n\\nvoid printFactors(int nu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CursorKind.FUNCTION_DECL printFactors\\n  Curso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Question  \\\n",
       "0           0  Print the factors of a number   \n",
       "1           1  Print the factors of a number   \n",
       "2           2  Print the factors of a number   \n",
       "3           3  Print the factors of a number   \n",
       "4           4  Print the factors of a number   \n",
       "\n",
       "                                        Correct_Code  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "3  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "4  #include <stdio.h>\\nvoid printFactors(int numb...   \n",
       "\n",
       "                                     Code_with_Error  Total_Marks  \\\n",
       "0  #include <stdio.h>\\nvoid printFactors(int numb...          7.0   \n",
       "1  #include <stdio.h>\\nvoid printFactors(int numb...          8.0   \n",
       "2  #include <stdio.h>\\nvoid printFactors(int numb...          5.0   \n",
       "3  #include <stdio.h>\\n\\nvoid printFactors(int nu...          7.0   \n",
       "4  #include <stdio.h>\\n\\nvoid printFactors(int nu...          5.0   \n",
       "\n",
       "                                            AST_full  \n",
       "0  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "1  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "2  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "3  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  \n",
       "4  CursorKind.FUNCTION_DECL printFactors\\n  Curso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading data\n",
    "data = pd.read_csv('Data_AST.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789c8bc",
   "metadata": {
    "id": "7789c8bc"
   },
   "source": [
    "### Code to full AST and partial AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62220727",
   "metadata": {
    "id": "62220727"
   },
   "outputs": [],
   "source": [
    "####IMPLEMENTATION OF C CODE PARSING USING SIMPLE AST\n",
    "\n",
    "import clang.cindex\n",
    "import tempfile\n",
    "\n",
    "def parse_c_code_from_string(c_code):\n",
    "\n",
    "    # Create a temporary file to store the C code string\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".c\", delete=False) as tmp_file:\n",
    "        tmp_file.write(c_code.encode())\n",
    "        tmp_file.flush()\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    index = clang.cindex.Index.create()\n",
    "    translation_unit = index.parse(tmp_file_path)\n",
    "\n",
    "    # Function to recursively build the AST string representation\n",
    "    def build_ast_string(node, indent=0):\n",
    "        if node.kind == clang.cindex.CursorKind.TRANSLATION_UNIT:\n",
    "            ast_str = \"\"\n",
    "            for child in node.get_children():\n",
    "                ast_str += build_ast_string(child, indent)\n",
    "            return ast_str\n",
    "        else:\n",
    "            ast_str = '  ' * indent + f\"{node.kind}\\n\"\n",
    "            for child in node.get_children():\n",
    "                ast_str += build_ast_string(child, indent + 1)\n",
    "            return ast_str\n",
    "\n",
    "    # Build the AST string starting from the root cursor\n",
    "    ast_representation = build_ast_string(translation_unit.cursor)\n",
    "\n",
    "    return ast_representation\n",
    "\n",
    "data['AST_full'] = data['Code_with_Error'].apply(parse_c_code_from_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903b1f6",
   "metadata": {
    "id": "f903b1f6"
   },
   "outputs": [],
   "source": [
    "####IMPLEMENTATION OF C CODE PARSING USING PARTIAL AST\n",
    "\n",
    "import clang.cindex\n",
    "import tempfile\n",
    "\n",
    "def find_parent_manually(root, target_node):\n",
    "        for child in root.get_children():\n",
    "            # Check if the target_node is among the children\n",
    "            if any(grandchild == target_node for grandchild in child.get_children()):\n",
    "                return child\n",
    "            # Recursively search in deeper children\n",
    "            parent = find_parent_manually(child, target_node)\n",
    "            if parent:\n",
    "                return parent\n",
    "        return None\n",
    "\n",
    "def parse_c_code_from_string(c_code):\n",
    "    # Create a temporary file to store the C code string\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".c\", delete=False) as tmp_file:\n",
    "        tmp_file.write(c_code.encode())\n",
    "        tmp_file.flush()\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Create an index for parsing\n",
    "    index = clang.cindex.Index.create()\n",
    "\n",
    "    # Parse the C code from the temporary file\n",
    "    translation_unit = index.parse(tmp_file_path)\n",
    "\n",
    "    # Function to get original code lines for display\n",
    "    original_lines = c_code.splitlines()\n",
    "\n",
    "    # Function to find siblings using Clang's built-in functionality\n",
    "    def find_next_brother(node):\n",
    "        parent = find_parent_manually(translation_unit.cursor, node)\n",
    "        if not parent:\n",
    "            return None\n",
    "\n",
    "        siblings = list(parent.get_children())\n",
    "        current_index = siblings.index(node)\n",
    "\n",
    "        # Return the next sibling if it exists\n",
    "        if current_index + 1 < len(siblings):\n",
    "            return siblings[current_index + 1]\n",
    "        else:\n",
    "            return find_next_brother(parent)\n",
    "\n",
    "     # Function to find if right sibling exist\n",
    "    def right_sibling_exist(node):\n",
    "        parent = find_parent_manually(translation_unit.cursor, node)\n",
    "        if not parent:\n",
    "            return None\n",
    "\n",
    "        siblings = list(parent.get_children())\n",
    "        current_index = siblings.index(node)\n",
    "\n",
    "        # Return the next sibling if it exists\n",
    "        if current_index + 1 < len(siblings):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    node_same_line=[]\n",
    "    # Recursive function to build AST string, printing errors and original lines when needed\n",
    "    def build_ast_string(node, indent=0):\n",
    "        ast_str = ''\n",
    "        # Check diagnostics at the current node\n",
    "        #for d in translation_unit.diagnostics:\n",
    "            #print(f\"Line: {d.location.line}, Column: {d.location.column}, Severity: {d.severity}, Message: {d.spelling}\")\n",
    "\n",
    "        diagnostics =[]\n",
    "        diagnostics_1=[]\n",
    "\n",
    "        for d in translation_unit.diagnostics:\n",
    "            if d.location.line == node.location.line and node.location.line not in node_same_line:\n",
    "                diagnostics.append(d)\n",
    "\n",
    "            elif d.location.line == node.location.line+1:\n",
    "                diagnostics_1.append(d)\n",
    "\n",
    "\n",
    "        if diagnostics and len(list(node.get_children()))==0 and not right_sibling_exist(node):\n",
    "            # If there's a diagnostic, print the error and the code causing it\n",
    "            ast_str += '  ' * indent + f\"{node.kind} {node.spelling}\\n\"\n",
    "            ast_str += '  ' * indent + f\"{original_lines[node.location.line - 1]}\\n\"\n",
    "            #This list ensures that if the error exist in the same line, the code is printed only once and not for all it's childen\n",
    "            node_same_line.append(node.location.line)\n",
    "        else:\n",
    "            # Continue building the AST normally\n",
    "            ast_str += '  ' * indent + f\"{node.kind} {node.spelling}\\n\"\n",
    "\n",
    "        # Find the next sibling to check lines between the current node and its sibling\n",
    "        sibling = find_next_brother(node)\n",
    "\n",
    "        # Check and print code lines between the current node and its sibling\n",
    "        if sibling and sibling.location.line - node.location.line > 1:\n",
    "            print(sibling.location.line - node.location.line)\n",
    "            # Print lines that are between the current node and its sibling\n",
    "            for line_num in range(node.location.line, sibling.location.line - 1):\n",
    "                line = original_lines[line_num]\n",
    "                if line.strip() and not all(char in ')}] ' for char in line):\n",
    "                    print(\"in\")\n",
    "                    ast_str += '  ' * indent + f\"{line}\\n\"\n",
    "\n",
    "        elif sibling is None and len(original_lines)-node.location.line > 1 and  len(list(node.get_children()))==0:\n",
    "            print(\"in\")\n",
    "            # Print lines that are between the current node and its sibling\n",
    "            for line_num in range(node.location.line, len(original_lines) - 1):\n",
    "                line = original_lines[line_num]\n",
    "                if line.strip() and not all(char in '(){}[] ' for char in line):\n",
    "                    ast_str += '  ' * indent + f\"{line}\\n\"\n",
    "\n",
    "        # Recursively traverse and build AST for child nodes\n",
    "        for child in node.get_children():\n",
    "            ast_str += build_ast_string(child, indent + 1)\n",
    "\n",
    "        return ast_str\n",
    "\n",
    "    # Build the AST string starting from the root cursor\n",
    "    ast_representation = build_ast_string(translation_unit.cursor)\n",
    "\n",
    "    return ast_representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec8a64",
   "metadata": {
    "id": "64ec8a64"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9716b769",
   "metadata": {
    "id": "9716b769"
   },
   "outputs": [],
   "source": [
    "def clean_text_code(text):\n",
    "        #Preprocesses text by removing blank lines, reducing whitespace, and replacing newlines with spaces.\n",
    "        text = '\\n'.join(line for line in text.split('\\n') if line.strip())  # Remove blank lines\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)  # Replace multiple spaces with one\n",
    "        return text.replace('\\n', ' ')  # Replace newlines with spaces\n",
    "\n",
    "\"\"\"\n",
    "def clean_text_AST(text):\n",
    "    text=str(text)\n",
    "    # Split the input into lines while preserving the structure\n",
    "    text = text.replace('CursorKind.', '')\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        leading_spaces = len(line) - len(line.lstrip())\n",
    "        stripped_line = line.lstrip()\n",
    "\n",
    "        # Replace 'CursorKind.' and process each word\n",
    "        words = stripped_line.split()\n",
    "        cleaned_words = []\n",
    "\n",
    "\n",
    "        for word in words:\n",
    "            if '_' in word:\n",
    "                parts = word.split('_')\n",
    "                cleaned_word = parts[0][:3]  # First 3 letters before the '_'\n",
    "\n",
    "                if len(parts) > 1:\n",
    "                    cleaned_word += parts[1][:3]  # Next 3 letters after the '_'\n",
    "\n",
    "                if len(parts) > 2:\n",
    "                    cleaned_word += parts[2][:2]  # Take 2 letters after the second '_'\n",
    "\n",
    "                cleaned_words.append(cleaned_word)\n",
    "            else:\n",
    "                cleaned_words.append(word[:4])  # Take the first 4 letters if no '_'\n",
    "\n",
    "        # Rejoin the cleaned words into a single line\n",
    "        cleaned_line = ' '.join(cleaned_words)\n",
    "\n",
    "        # Reapply the leading spaces (indentation) to the line\n",
    "        cleaned_lines.append(' ' * leading_spaces + cleaned_line)\n",
    "\n",
    "    # Join the cleaned lines back into a single string, preserving newlines\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    text = re.sub(r'\\s{2,}', ' ', cleaned_text)  # Replace multiple spaces with one\n",
    "    return text.replace('\\n', ' ')\n",
    "\"\"\"\n",
    "\n",
    "#data['AST_full_Processed']=data['AST_full'].apply(clean_text_AST)\n",
    "data['Code_with_Error_Processed']=data['Code_with_Error'].apply(clean_text_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4caefe",
   "metadata": {
    "id": "0e4caefe"
   },
   "source": [
    "### CodeBERT embedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b0539ea-9973-4404-bf64-d4c4a6602859",
   "metadata": {
    "id": "7b0539ea-9973-4404-bf64-d4c4a6602859"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class CodeT5Embeddings:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "        self.model = AutoModel.from_pretrained(\"Salesforce/codet5-base\").to(self.device)\n",
    "          # Add BiLSTM\n",
    "        self.bilstm = torch.nn.LSTM(\n",
    "            input_size=768,  # CodeT5's embedding size\n",
    "            hidden_size=512,  # BiLSTM hidden size\n",
    "            num_layers=1,  # Number of LSTM layers\n",
    "            bidirectional=True,  # BiLSTM for forward and backward passes\n",
    "            batch_first=True  # Input shape (batch_size, seq_length, input_size)\n",
    "        ).to(self.device)\n",
    "        \n",
    "    # def forward(self, inputs):\n",
    "    #     # LSTM forward pass\n",
    "    #     lstm_output, (hidden_state, cell_state) = self.bilstm(inputs)\n",
    "    #     return lstm_output\n",
    "\n",
    "    # Function to chunk the input text into 512-token segments\n",
    "    def chunk_text(self, text, max_length=512):\n",
    "        # Tokenize the entire text first\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=False, padding=False)\n",
    "\n",
    "        input_ids = tokens['input_ids'][0]  # The tokenized sequence\n",
    "        chunks = [input_ids[i:i + max_length] for i in range(0, len(input_ids), max_length)]\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def _get_embeddings(self, text_batch):\n",
    "        if not text_batch:  # Handle empty input\n",
    "            logging.warning(f\"Empty input text batch provided.\")\n",
    "            return torch.zeros(0, 768).to(self.device)\n",
    "\n",
    "        embeddings_per_text = []\n",
    "\n",
    "        # Step 1: Process each text separately\n",
    "        for text in text_batch:\n",
    "            # Step 2: Tokenize and chunk the text\n",
    "            chunked_text = self.chunk_text(text)\n",
    "\n",
    "\n",
    "            # Step 4: Run the model on each chunk separately\n",
    "            all_chunk_embeddings = []\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.encoder(**chunked_text)  # Model inference\n",
    "                #chunk_embeddings = outputs.last_hidden_state[:, 0, :]  # Get embeddings for [CLS] token of each chunk\n",
    "                all_chunk_embeddings.append(outputs.last_hidden_state)\n",
    "            # Concatenate all chunks into a single tensor for LSTM\n",
    "            concatenated_embeddings = torch.cat(all_chunk_embeddings, dim=1)  # (1, seq_len, 768)\n",
    "            BiLSTM, _  = self.bilstm(all_chunk_embeddings.unsqueeze(0))\n",
    "\n",
    "            # Step 5: Concatenate the embeddings for all chunks of the text\n",
    "            lstm_output, _ = self.bilstm(concatenated_embeddings)\n",
    "\n",
    "            embeddings_per_text.append(lstm_output)\n",
    "\n",
    "        # Clear cache after processing the embedding\n",
    "        del tokens, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return embeddings_per_text #Array of arrays containing each array of 512 tokens embedded using T5\n",
    "\n",
    "    def process_embeddings_in_batches(self, texts, batch_size=16):\n",
    "\n",
    "        all_embeddings = []\n",
    "\n",
    "        # Divide the list into batches of size `batch_size`\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            # Get embeddings for the current batch\n",
    "            batch_embeddings = self._get_embeddings(batch_texts)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Combine embeddings from all batches\n",
    "        final_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "        # Clear memory\n",
    "        del all_embeddings, batch_embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return final_embeddings\n",
    "\n",
    "    def process(self, code, ast_full):\n",
    "        code_embedding = self.process_embeddings_in_batches(code)\n",
    "        ast_embedding = self.process_embeddings_in_batches(ast_full)\n",
    "\n",
    "        # More cache clearing after each embedding\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return code_embedding, ast_embedding\n",
    "\n",
    "\n",
    "class CodeT5Processor:\n",
    "    def __init__(self, dataframe, embedding_model, embedding_save_path=\"embeddings.pt\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embedding_save_path = embedding_save_path\n",
    "        self.embeddings = None  # To store the embeddings\n",
    "\n",
    "    def compute_embeddings(self):\n",
    "        code_embeddings = []\n",
    "        ast_embeddings = []\n",
    "        full_code = []\n",
    "        full_ast = []\n",
    "        updated_rows = []\n",
    "        i=0\n",
    "        for idx, row in self.dataframe.iterrows():\n",
    "\n",
    "            code = row['Code_with_Error_Processed']\n",
    "            ast = row['AST_full_Processed']\n",
    "\n",
    "            # Check if either tokenized code or AST exceeds the length of 512 tokens\n",
    "            code_tokens = self.embedding_model.tokenizer(code, return_tensors=\"pt\")['input_ids']\n",
    "            ast_tokens = self.embedding_model.tokenizer(ast, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "            # Log and skip rows where token size is greater than 512 for either code or AST\n",
    "            if len(code_tokens[0]) > 512 or len(ast_tokens[0]) > 512:\n",
    "                i+=1\n",
    "                continue\n",
    "\n",
    "            full_code.append(str(code))\n",
    "            full_ast.append(str(ast))\n",
    "            updated_rows.append(row.to_dict())\n",
    "\n",
    "        new_dataframe = pd.DataFrame(updated_rows)\n",
    "        new_dataframe.to_csv('updated_data.csv', index=False)\n",
    "        print(f\"Number of rows skipped due to token size exceeding 512: {i}\")\n",
    "\n",
    "        # Compute embeddings for code and AST\n",
    "        code_embedding, ast_embedding = self.embedding_model.process(full_code, full_ast)\n",
    "\n",
    "        # Append to lists and clear memory after processing each\n",
    "        code_embeddings.append(code_embedding.cpu())\n",
    "        ast_embeddings.append(ast_embedding.cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Store the embeddings\n",
    "        self.embeddings = {\n",
    "            'code_embeddings': torch.stack(code_embeddings),\n",
    "            'ast_embeddings': torch.stack(ast_embeddings)\n",
    "        }\n",
    "\n",
    "    def save_embeddings(self):\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No embeddings found. Please compute embeddings first.\")\n",
    "        torch.save(self.embeddings, self.embedding_save_path)\n",
    "        print(f\"Embeddings saved to {self.embedding_save_path}\")\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        self.embeddings = torch.load(self.embedding_save_path)\n",
    "        print(f\"Embeddings loaded from {self.embedding_save_path}\")\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No embeddings found. Please compute or load embeddings first.\")\n",
    "        return self.embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7-D1R4EI5NZ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7-D1R4EI5NZ7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "df1fbad9-ca4e-4cc8-b1d9-c3308cdbec88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with more than 512 tokens: 317\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "# Tokenization length check\n",
    "def count_exceeding_rows(column, max_tokens=1024):\n",
    "    exceeding_rows = 0\n",
    "    for row in column:\n",
    "\n",
    "        tokenized_length = len(tokenizer(str(row), return_tensors=\"pt\")['input_ids'][0])\n",
    "        if tokenized_length > max_tokens:\n",
    "            exceeding_rows += 1\n",
    "    return exceeding_rows\n",
    "\n",
    "# Check how many rows exceed 512 tokens\n",
    "rows_exceeding = count_exceeding_rows(data['AST_full'], max_tokens=1024)\n",
    "print(f\"Number of rows with more than 512 tokens: {rows_exceeding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8YN9NHCEQ5vz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YN9NHCEQ5vz",
    "outputId": "5b6aa90b-a07a-408c-ea15-a62af4c18c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CursorKind.FUNCTION_DECL\n",
      "  CursorKind.PARM_DECL\n",
      "  CursorKind.COMPOUND_STMT\n",
      "    CursorKind.CALL_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.UNEXPOSED_EXPR\n",
      "          CursorKind.STRING_LITERAL\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "    CursorKind.FOR_STMT\n",
      "      CursorKind.DECL_STMT\n",
      "        CursorKind.VAR_DECL\n",
      "          CursorKind.INTEGER_LITERAL\n",
      "      CursorKind.BINARY_OPERATOR\n",
      "        CursorKind.UNEXPOSED_EXPR\n",
      "          CursorKind.DECL_REF_EXPR\n",
      "        CursorKind.UNEXPOSED_EXPR\n",
      "          CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.UNARY_OPERATOR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.COMPOUND_STMT\n",
      "        CursorKind.IF_STMT\n",
      "          CursorKind.BINARY_OPERATOR\n",
      "            CursorKind.BINARY_OPERATOR\n",
      "              CursorKind.UNEXPOSED_EXPR\n",
      "                CursorKind.DECL_REF_EXPR\n",
      "              CursorKind.UNEXPOSED_EXPR\n",
      "                CursorKind.DECL_REF_EXPR\n",
      "            CursorKind.INTEGER_LITERAL\n",
      "          CursorKind.COMPOUND_STMT\n",
      "            CursorKind.CALL_EXPR\n",
      "              CursorKind.UNEXPOSED_EXPR\n",
      "                CursorKind.DECL_REF_EXPR\n",
      "              CursorKind.UNEXPOSED_EXPR\n",
      "                CursorKind.UNEXPOSED_EXPR\n",
      "                  CursorKind.STRING_LITERAL\n",
      "              CursorKind.UNEXPOSED_EXPR\n",
      "                CursorKind.DECL_REF_EXPR\n",
      "    CursorKind.CALL_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.UNEXPOSED_EXPR\n",
      "          CursorKind.STRING_LITERAL\n",
      "CursorKind.FUNCTION_DECL\n",
      "  CursorKind.COMPOUND_STMT\n",
      "    CursorKind.DECL_STMT\n",
      "      CursorKind.VAR_DECL\n",
      "    CursorKind.CALL_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.UNEXPOSED_EXPR\n",
      "          CursorKind.STRING_LITERAL\n",
      "    CursorKind.CALL_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.UNEXPOSED_EXPR\n",
      "          CursorKind.STRING_LITERAL\n",
      "      CursorKind.UNARY_OPERATOR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "    CursorKind.CALL_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "      CursorKind.UNEXPOSED_EXPR\n",
      "        CursorKind.DECL_REF_EXPR\n",
      "    CursorKind.RETURN_STMT\n",
      "      CursorKind.INTEGER_LITERAL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data['AST_full'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b8544",
   "metadata": {
    "id": "857b8544"
   },
   "source": [
    "### Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8267ee2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "c8267ee2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5d81d753-a047-4afa-afda-dea635442da2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows skipped due to token size exceeding 512: 133\n",
      "Embeddings saved to embeddings.pt\n",
      "Embeddings loaded from embeddings.pt\n",
      "tensor([[[ 0.0041, -0.2031, -0.1294,  ..., -0.0496, -0.0128, -0.1429],\n",
      "         [-0.0053, -0.2308, -0.1260,  ..., -0.0506, -0.0143, -0.1440],\n",
      "         [ 0.0149, -0.2211, -0.1231,  ..., -0.0525, -0.0129, -0.1247],\n",
      "         ...,\n",
      "         [ 0.0077, -0.0115, -0.0123,  ..., -0.0076,  0.0037, -0.0278],\n",
      "         [ 0.0019, -0.0086, -0.0080,  ..., -0.0049,  0.0005, -0.0138],\n",
      "         [-0.0029, -0.2452, -0.1435,  ..., -0.0112, -0.0482, -0.1252]]])\n",
      "tensor([[[ 1.9006e-03, -5.4137e-03,  2.2087e-03,  ...,  2.9823e-03,\n",
      "           8.3012e-04, -8.0320e-03],\n",
      "         [ 1.8753e-03, -5.2868e-03,  1.9338e-03,  ...,  2.6454e-03,\n",
      "           6.4081e-04, -7.6128e-03],\n",
      "         [ 1.4670e-03, -5.2749e-03,  2.4984e-03,  ...,  3.0852e-03,\n",
      "          -1.5118e-05, -8.5196e-03],\n",
      "         ...,\n",
      "         [ 1.4230e-03, -3.4453e-03,  6.6530e-04,  ...,  3.8998e-03,\n",
      "           3.5129e-04, -7.6585e-03],\n",
      "         [ 2.0752e-03, -3.7569e-03, -4.6707e-05,  ...,  2.6769e-03,\n",
      "           1.8415e-03, -8.5095e-03],\n",
      "         [ 2.6523e-03, -5.0551e-03,  9.3081e-04,  ...,  1.4553e-03,\n",
      "           1.9078e-03, -8.4490e-03]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-99d5b5c96c17>:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.embeddings = torch.load(self.embedding_save_path)\n"
     ]
    }
   ],
   "source": [
    "#Embedding extraction\n",
    "embedding_model = CodeT5Embeddings()\n",
    "processor = CodeT5Processor(data, embedding_model)\n",
    "processor.compute_embeddings()\n",
    "processor.save_embeddings()\n",
    "processor.load_embeddings()\n",
    "embeddings = processor.get_embeddings()\n",
    "\n",
    "print(embeddings['code_embeddings'])\n",
    "print(embeddings['ast_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "PRMvnHoU6NH1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRMvnHoU6NH1",
    "outputId": "4afdad3e-ca41-440c-c18d-227f7222fb62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings['code_embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "xBDsDd7u6RhP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBDsDd7u6RhP",
    "outputId": "65c58a4e-4ba9-409b-8d9b-f1627512c775"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings['ast_embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "yfuxQ6bj6Wrg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfuxQ6bj6Wrg",
    "outputId": "4e523ef2-a896-4ec0-b4f5-8ac7e3d013aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "<ipython-input-26-b15cf4eae5cd>:18: RuntimeWarning: divide by zero encountered in divide\n",
      "  mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Embeddings - RMSE: 1.8038, R²: 0.3255, MAPE: inf%\n",
      "AST Embeddings - RMSE: 2.1036, R²: 0.0827, MAPE: inf%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "<ipython-input-26-b15cf4eae5cd>:18: RuntimeWarning: divide by zero encountered in divide\n",
      "  mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "updated_data = pd.read_csv('updated_data.csv')\n",
    "print(len(updated_data['Total_Marks']))\n",
    "# Embeddings\n",
    "code_embeddings = np.array(embeddings['code_embeddings'][0])\n",
    "ast_embeddings = np.array(embeddings['ast_embeddings'][0])\n",
    "\n",
    "# Ground truth scores\n",
    "y = updated_data['Total_Marks'].values\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)  # Root Mean Squared Error\n",
    "    r2 = r2_score(y_true, y_pred)  # R-squared\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n",
    "    return rmse, r2, mape\n",
    "\n",
    "\n",
    "########## CODE TRAINING ############\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(code_embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xg_reg_code = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6\n",
    ")\n",
    "xg_reg_code.fit(X_train, y_train)\n",
    "\n",
    "y_pred_code = xg_reg_code.predict(X_test)\n",
    "rmse_code, r2_code, mape_code = calculate_metrics(y_test, y_pred_code)\n",
    "\n",
    "print(f\"Code Embeddings - RMSE: {rmse_code:.4f}, R²: {r2_code:.4f}, MAPE: {mape_code:.4f}%\")\n",
    "\n",
    "\n",
    "############ AST TRAINING ############\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ast_embeddings, y, test_size=0.2, random_state=42)\n",
    "xg_reg_ast = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6\n",
    ")\n",
    "xg_reg_ast.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_ast = xg_reg_ast.predict(X_test)\n",
    "rmse_ast, r2_ast, mape_ast = calculate_metrics(y_test, y_pred_ast)\n",
    "\n",
    "print(f\"AST Embeddings - RMSE: {rmse_ast:.4f}, R²: {r2_ast:.4f}, MAPE: {mape_ast:.4f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "PZVLgrhY9gHC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "PZVLgrhY9gHC",
    "outputId": "514eed82-0aa1-4ea5-fb78-c85ae0817d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9186857731d3>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Code Embeddings - 10-fold CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load updated data\n",
    "updated_data = pd.read_csv('updated_data.csv')\n",
    "print(len(updated_data['Total_Marks']))\n",
    "\n",
    "# Embeddings\n",
    "code_embeddings = np.array(embeddings['code_embeddings'])  # Note: embedding for all rows\n",
    "ast_embeddings = np.array(embeddings['ast_embeddings'])  # Note: embedding for all rows\n",
    "\n",
    "# Ground truth scores\n",
    "y = updated_data['Total_Marks'].values\n",
    "\n",
    "# Initialize lists to store results\n",
    "code_preds = []\n",
    "ast_preds = []\n",
    "\n",
    "# Define the cross-validation strategy (10-fold)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)  # Root Mean Squared Error\n",
    "    r2 = r2_score(y_true, y_pred)  # R-squared\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Code Embeddings - 10-fold CV\n",
    "for train_index, test_index in kf.split(code_embeddings[0]):\n",
    "    X_train, X_test = code_embeddings[train_index], code_embeddings[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xg_reg_code = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "    xg_reg_code.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_code = xg_reg_code.predict(X_test)\n",
    "    rmse_code, r2_code, mape_code = calculate_metrics(y_test, y_pred_code)\n",
    "\n",
    "    print(f\"Code Embeddings - RMSE: {rmse_code:.4f}, R²: {r2_code:.4f}, MAPE: {mape_code:.4f}%\")\n",
    "\n",
    "    # Store predictions for later use\n",
    "    code_preds.extend(y_pred_code)\n",
    "\n",
    "# AST Embeddings - 10-fold CV\n",
    "for train_index, test_index in kf.split(ast_embeddings):\n",
    "    X_train, X_test = ast_embeddings[train_index], ast_embeddings[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xg_reg_ast = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "    xg_reg_ast.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_ast = xg_reg_ast.predict(X_test)\n",
    "    rmse_ast, r2_ast, mape_ast = calculate_metrics(y_test, y_pred_ast)\n",
    "\n",
    "    print(f\"AST Embeddings - RMSE: {rmse_ast:.4f}, R²: {r2_ast:.4f}, MAPE: {mape_ast:.4f}%\")\n",
    "\n",
    "    # Store predictions for later use\n",
    "    ast_preds.extend(y_pred_ast)\n",
    "\n",
    "# After 10-fold CV, save the results back into the DataFrame\n",
    "updated_data['Predicted_Code_Score'] = code_preds\n",
    "updated_data['Predicted_AST_Score'] = ast_preds\n",
    "\n",
    "# Save the updated DataFrame with predictions\n",
    "updated_data.to_csv('updated_data_with_predictions.csv', index=False)\n",
    "print(\"Predictions have been added to the DataFrame and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "niO8YEFrAXyc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "niO8YEFrAXyc",
    "outputId": "eaeb046a-ca00-4fcb-9dc1-ffba1b86c383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Processing fold 2...\n",
      "Processing fold 3...\n",
      "Processing fold 4...\n",
      "Processing fold 5...\n",
      "Processing fold 6...\n",
      "Processing fold 7...\n",
      "Processing fold 8...\n",
      "Processing fold 9...\n",
      "Processing fold 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a25d208ddcc3>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Now calculate metrics after all folds are completed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mrmse_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmape_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mrmse_ast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_ast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmape_ast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_ast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a25d208ddcc3>\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Root Mean Squared Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# R-squared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m  \u001b[0;31m# Mean Absolute Percentage Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load updated data\n",
    "updated_data = pd.read_csv('updated_data.csv')\n",
    "\n",
    "# Embeddings for both code and AST (assuming these are loaded as 3D arrays)\n",
    "code_embeddings = np.array(embeddings['code_embeddings'][0])\n",
    "ast_embeddings = np.array(embeddings['ast_embeddings'][0])\n",
    "\n",
    "# Ground truth scores\n",
    "y = updated_data['Total_Marks'].values\n",
    "\n",
    "# Reshaping embeddings to 2D (flatten sequence dimension for XGBoost)\n",
    "code_embeddings_flat = code_embeddings.reshape(code_embeddings.shape[0], -1)  # Flattening the sequence\n",
    "ast_embeddings_flat = ast_embeddings.reshape(ast_embeddings.shape[0], -1)  # Flattening the sequence\n",
    "\n",
    "# Initialize KFold cross-validation with 10 splits\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare to store the predictions\n",
    "predictions_code = []\n",
    "predictions_ast = []\n",
    "y_true_all = []  # To store all true values\n",
    "\n",
    "# Define the evaluation metric function\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)  # Root Mean Squared Error\n",
    "    r2 = r2_score(y_true, y_pred)  # R-squared\n",
    "    #mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Mean Absolute Percentage Error\n",
    "    return rmse, r2#, mape\n",
    "\n",
    "# Perform 10-fold cross-validation for both embeddings (code and AST)\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(code_embeddings_flat)):\n",
    "    print(f\"Processing fold {fold + 1}...\")\n",
    "\n",
    "    # Splitting data into training and testing sets\n",
    "    X_train_code, X_test_code = code_embeddings_flat[train_index], code_embeddings_flat[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Training XGBoost model for code embeddings\n",
    "    xg_reg_code = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "    xg_reg_code.fit(X_train_code, y_train)\n",
    "\n",
    "    # Predicting for code embeddings\n",
    "    y_pred_code = xg_reg_code.predict(X_test_code)\n",
    "    predictions_code.extend(y_pred_code)  # Collect predictions for the code model\n",
    "\n",
    "    # Repeat the same for AST embeddings\n",
    "    X_train_ast, X_test_ast = ast_embeddings_flat[train_index], ast_embeddings_flat[test_index]\n",
    "\n",
    "    # Train XGBoost model for AST embeddings\n",
    "    xg_reg_ast = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "    xg_reg_ast.fit(X_train_ast, y_train)\n",
    "\n",
    "    # Predicting for AST embeddings\n",
    "    y_pred_ast = xg_reg_ast.predict(X_test_ast)\n",
    "    predictions_ast.extend(y_pred_ast)  # Collect predictions for the AST model\n",
    "\n",
    "    # Collect true values for later evaluation\n",
    "    y_true_all.extend(y_test)\n",
    "\n",
    "# Now calculate metrics after all folds are completed\n",
    "rmse_code, r2_code, mape_code = calculate_metrics(y_true_all, predictions_code)\n",
    "rmse_ast, r2_ast, mape_ast = calculate_metrics(y_true_all, predictions_ast)\n",
    "\n",
    "print(f\"Final Evaluation - Code Embeddings: RMSE: {rmse_code:.4f}, R²: {r2_code:.4f}, MAPE: {mape_code:.4f}%\")\n",
    "print(f\"Final Evaluation - AST Embeddings: RMSE: {rmse_ast:.4f}, R²: {r2_ast:.4f}, MAPE: {mape_ast:.4f}%\")\n",
    "\n",
    "# Save predictions as new columns in the dataframe\n",
    "updated_data['predicted_code'] = predictions_code\n",
    "updated_data['predicted_ast'] = predictions_ast\n",
    "\n",
    "# Save the updated dataframe with predictions\n",
    "updated_data.to_csv('updated_data_with_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "oO4ZU20_EdYj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oO4ZU20_EdYj",
    "outputId": "0da275f8-375a-4db1-d8fd-c367323a1037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation - Code Embeddings: RMSE: 1.8706, R²: 0.2625, MAPE: 33.8853%\n",
      "Final Evaluation - AST Embeddings: RMSE: 2.0389, R²: 0.1239, MAPE: 37.5442%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluation metric function\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Filter out rows where true value is 0 to avoid MAPE going to infinity\n",
    "    mask = y_true != 0\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "\n",
    "    rmse = mean_squared_error(y_true_filtered, y_pred_filtered, squared=False)  # Root Mean Squared Error\n",
    "    r2 = r2_score(y_true_filtered, y_pred_filtered)  # R-squared\n",
    "    mape = np.mean(np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)) * 100  # Mean Absolute Percentage Error\n",
    "    return rmse, r2, mape\n",
    "\n",
    "y_true_all = np.array(y_true_all)\n",
    "predictions_code = np.array(predictions_code)\n",
    "predictions_ast = np.array(predictions_ast)\n",
    "\n",
    "# Now calculate metrics after all folds are completed\n",
    "rmse_code, r2_code, mape_code = calculate_metrics(y_true_all, predictions_code)\n",
    "rmse_ast, r2_ast, mape_ast = calculate_metrics(y_true_all, predictions_ast)\n",
    "\n",
    "print(f\"Final Evaluation - Code Embeddings: RMSE: {rmse_code:.4f}, R²: {r2_code:.4f}, MAPE: {mape_code:.4f}%\")\n",
    "print(f\"Final Evaluation - AST Embeddings: RMSE: {rmse_ast:.4f}, R²: {r2_ast:.4f}, MAPE: {mape_ast:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "oaI8nMBCA5nT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaI8nMBCA5nT",
    "outputId": "3359bbac-8a12-4d83-e05f-e1d6dcf382b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1i_R_LuQE_Tr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1i_R_LuQE_Tr",
    "outputId": "25c783f7-21a7-4b3b-8ca2-125abe46fb3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9lsCR49FDgQ",
   "metadata": {
    "id": "l9lsCR49FDgQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7789c8bc"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
